%! Author = zamoosh
%! Date = 6/10/23

\chapter{رسانه‌های‌مصنوعی و خشونت سیاسی}
\label{ch:رسانه‌های‌مصنوعی و خشونت سیاسی}
\phantomsection

\begin{quote}
    بنابراین رسانه‌های ترکیبی ظرفیت تخریب سرمایه‌ی اجتماعی و اعتبار را در پایه خود دارند، و این به همان اندازه صادق است که آیا محتوای آن‌ها را درست می‌پذیریم یا نه.
    مشکل اساسی‌تر این است که ما هیچ معیار عینی‌ای برای تعیین اینکه چه چیزی شایسته باور است نداریم.
    علاوه بر این، جلب توجه به پدیده رسانه‌های‌مصنوعی تنها تأثیرات آن را تقویت می‌کند و برای ما چاره‌ای جز بازگشت به ترجیحات، تعصبات و ایدئولوژی‌های سیاسی خود باقی نمی‌گذارد.
    \\\\
    \textbf{تریسی داودزول و شان گولتز \textenglish{\textbf{(Tracey Dowdeswell and Sean Goltz)}}}
    \newline
\end{quote}


{\setstretch{0.5}
\phantomsection
\section*{کودتا در گابن}
\label{sec:کودتا در گابن}
\addcontentsline{toc}{section}{کودتا در گابن}{\protect\numberline{}}
«گابن»، کشوری نسبتاً باثبات در سواحل غربی آفریقا است، در روزهای ابتدایی سال 2019 توسط یک کودتا لرزید. جرقه‌ی این کودتا تا حد زیادی توسط یک سخنرانی معمولی در شب سال‌نو توسط «رئیس جمهور علی بونگو اودیمبا» آغاز شد که در شبکه های اجتماعی منتشر شده‌بود. رئیس جمهور مدتی بود که از انظار عمومی دور بود (در واقع او بیش از دو ماه بود که در کشور نبود) و شایعاتی مبنی بر اینکه او به شدت بیمار است یا حتی مرده است، در شبکه‌های اجتماعی پیچیده بود.
}

%   \textenglish{\textbf{}}


پیام سال‌نو به طور گسترده‌ای به عنوان یک «\textenglish{\textbf{deepfake}}» محکوم شد.
در حالی که پیام سال نو به خودی خود غیرقابل توجه بود، نحوه نمایش علی بونگو در این ویدئو کاملاً عجیب است: چهره رئیس جمهور بالای دهانش به طرز عجیبی بی حرکت است.
او به مدت 1 دقیقه و 39 ثانیه پس از فیلم، چشمانش اصلاً پلک نمی‌خورند و گفتار و حرکات او غیرطبیعی و مصنوعی به نظر می‌رسد.
نظرات به این ویدیو نشان می‌دهد که بینندگان احساس می‌کردند که این ویدیو «وحشتناک» و «\textenglish{\textbf{deepfake}}» است.
در 3 ژانویه 2019، یک رسانه‌ی خبری برجسته در گابن، مقاله‌ای را منتشر کرد که آشکارا این ویدئو را یک \textenglish{\textbf{deepfake}} محکوم کرد.

"رسانه مصنوعی" اصطلاحی فنی است که برای توصیف آنچه اغلب "\textenglish{\textbf{deepfake}}" نامیده می‌شود استفاده می‌شود.
رسانه‌های مصنوعی از یادگیری ماشین و شبکه‌های عصبی برای ایجاد رسانه‌های صوتی، عکس، ویدیو یا حتی متن مصنوعی استفاده می‌کنند که معتبر به نظر می‌رسند.
فیلم ویدئویی را می‌توان تغییر داد تا گفتار و حرکات یک نفر را بر روی صحبت‌های شخص دیگر قرار دهد.
رسانه‌های مصنوعی فراتر از فیلم‌های عکس و ویدیو هستند و شامل گفتار و نوشتار تولید شده توسط هوش‌مصنوعی می‌شوند.
این روش‌ها برای تولید حجم فزاینده‌ای از محتوایی که در اینترنت می‌بینیم استفاده می‌شوند و توسط «مزدوران» برای تولید ایمیل‌ها، متن‌ها یا پیام‌های نادرست که به نظر می‌رسد از سوی افرادی هستند که شما را به خوبی می‌شناسند، استفاده می‌شوند.
متن مصنوعیِ ساخته‌شده توسط هوش‌مصنوعی، حتی برای تولید مقالات آکادمیک نیز استفاده شده است؛ و جالب است که بدایند آن مقاله به چاپ نیز رسیده‌است!
چندین ادعا در مجله \textenglish{\textbf{\mbox{Arabian Journal of Geosciences}}} به دلیل بسیار غیرمعمولی، به 10 ادعای برتر ایوان اورانسکی در سال 2021 تبدیل شد؛ آن‌ها چرندیات محض بودند!
[توضیح ادعای اورانسکی: Retraction Watch، وبلاگی است که در مورد ادعای مقالات علمی و موضوعات مرتبط گزارش می‌دهد.
این وبلاگ در آگوست 2010 راه‌اندازی شد و توسط نویسندگان علم، ایوان اورانسکی (معاون سابق، \textenglish{\textbf{\mbox{Editorial Medscape}}}) و آدام مارکوس (ویرایشگر اخبار گوارش و اندوسکوپی) تولید می‌شود].
یک مقاله باید از مجله \textenglish{\textbf{Arabian Journal of Geosciences}} به این دلیل غیرعادی که «محتوای این مقاله مزخرف است» پس گرفته شود.
به نظر می‌رسد این مشکل بسیار گسترده شده است: بیش از 400 مقاله در مجلات متعلق به \textenglish{\textbf{Springer Nature}} و صدها مقاله‌ی دیگر در \textenglish{\textbf{Elsevier}} نیز آلوده به متن مصنوعی هستند!
فناوری‌های رسانه‌ی‌مصنوعی سریع‌تر از فناوری‌هایی که برای شناسایی آن‌ها استفاده می‌شوند توسعه می‌یابند و \textenglish{\textbf{deepfake}} را تقریباً نامرئی می‌کنند.

نیروهای مسلح گابن (که مدت‌ها در مخالفت با حزب حاکم بونگو بودند) با ویدیوی سال نو رئیس جمهور نیز احساس کردند که به تمسخر گرفته شده‌اند.
در ساعات اولیه صبح 7 ژانویه 2019، «ستوان کِلی اوندو اوبیانگ» از گارد جمهوری خواه اعلام کرد که سخنرانی سال نو نشان داد که «علی بونگو» برای اداره کشور مناسب نیست.
بر این اساس، ارتش او را از سمت خود برکنار کرد و می‌خواست «شورای بازسازی» ملی را برای حکومت به جای او تشکیل دهد.
تانک‌ها وارد لیبرویل (پایتخت) شدند.
اینترنت و برق قطع شد.
حدود 300 معترض به حمایت از کودتا آمدند و توسط نیروهای دولتی با گاز اشک آور مورد حمله قرار گرفتند.
صدای تیراندازی در پایتخت شنیده شد.
تا ساعت 10 صبح، نیروهای دولتی دوباره کنترل شدند و رهبران کودتا یا مرده بودند یا در بازداشت بودند.

کارشناسان هوش مصنوعی شروع به آزمایش کردند که آیا ویدیوی سال نو ساخته‌شده‌است یا خیر، و به سرعت یک اجماع واضح ظاهر شد.\ «استیو گروبمن (\textenglish{\textbf{Steve Grobman}})» (مدیر ارشد فناوری \textenglish{\textbf{McAfee}}) ویدیو را از طریق الگوریتم‌های آن‌ها اجرا کرد و با احتمال بسیار بالا (92٪) تشخیص داد که ویدیو واقعی است.
سیوی لیو (\textenglish{\textbf{Siwei Lyu}})، پروفسور علوم کامپیوتر در «\textenglish{\textbf{\mbox{SUNY Albany}}}»، همچنین ویدیو را با استفاده از الگوریتم \textenglish{\textbf{deepfake}} خود بررسی کرد و تائید کرد که به احتمال 99٪ ویدئو واقعی است.
الگوریتم‌ها، هیچ مدرکی مبنی بر استفاده از روش‌های شناخته‌شده‌ی مصنوعی‌کردن رسانه، برای تولید ویدئو پیدا نکردند.
اگر این یک \textenglish{\textbf{deepfake}} بود، باید یک فرضیه بسیار پیچیده بوده باشد (فرضیه‌ای که توسط عجیب بودن خود ویدیو پشتیبانی نمی‌شود).

صحت این ویدیو با مشاهده خود علی بونگو در حضورهای عمومی بعدی تأیید‌شد که نشان می‌دهد او زنده است (اما همچنین بسیار تغییر یافته است).
«الکساندر درومریک» (\textenglish{\textbf{Alexander Dromeric}})، متخصص مغز و اعصاب واشینگتن پست، اظهار داشت که حرکات و بی‌تحرکی صورت علی بونگو مشخصه افرادی است که سکته کرده یا دچار نوعی آسیب مغزی شده‌اند.
از آن زمان دفتر رسمی مطبوعاتی دولت از تایید یا تکذیب اینکه آیا علی بونگو دچار سکته مغزی شده است خودداری کرده است.
دولت گابن در واقع به اطلاعات نادرست و بی اعتمادی در مورد سلامت رئیس جمهور دامن می زد (اما به دلیل عدم شفافیت آن‌ها و نه با تولید رسانه های مصنوعی).
مشکل \textenglish{\textbf{deepfake}} فقط این نیست که اطلاعات نادرست تولید می‌کنند، بلکه وجود آن‌ها مردم را به سمت بی اعتبار کردن گزارش‌هایی سوق می‌دهد که در واقع درست هستند.

«آویو اوادیا» (\textenglish{\textbf{Aviv Ovadya}})، متخصص هوش‌مصنوعی و رسانه‌های مصنوعی، بیان می‌کند که \textenglish{\textbf{deepfake}} می‌تواند بسیار خطرناک باشد، دقیقاً به این دلیل که بی اعتمادی گسترده نسبت به همه‌ی رسانه‌ها ایجاد می‌کند.
این باعث ایجاد نوعی «بی‌تفاوتی واقعیت» می‌شود که اوادیا آن را «\textenglish{\textbf{Infocalypse}}» نامیده است (از دست دادن اساسی اعتماد به نهادهای اجتماعی).
یک \textenglish{\textbf{Infocalypse}} زمانی به وجود می‌آید که متوجه می‌شویم استانداردهایی برای حقیقت و عینیت نداریم و کنترلی بر فناوری‌های در حال تکامل سریع نداریم، همانطور که در گابن اتفاق افتاد.
اوادیا می‌گوید که «مخاطره‌ها زیاد است و پیامدهای احتمالی فاجعه‌بارتر از مداخله خارجی در انتخابات است («\textenglish{\textbf{Infocalypse}}»: تضعیف یا فروپاشی نهادهای اصلی).

بی‌تفاوتی واقعی که توسط رسانه‌های‌مصنوعی ایجاد می‌شود پرهزینه است (همانطور که اوادیا می‌گوید، «هم برای سازمان‌های رسانه‌ای که مجبور به صرف زمان و منابع برای بررسی و شناسایی جعلی بودن این شکل ویدیوها هستند هم برای جوامعی که در بحث‌هایی درباره اصالت آن‌ها صحبت می‌کنند»).
برای گابن، این هزینه‌ها در خشونت سیاسی، افزایش ترس، گسست اجتماعی و مرگ دو عضو نیروهای مسلح شورشی دیده‌شد.
\newline
\newline



{\setstretch{0.5}
\phantomsection
{\setstretch{1}
\section*{بولی بای \textenglish{\textbf{(Bulli Bai)}}: فروش زنان به صورت مصنوعی در هند}
}
\label{sec:بولی بای (Bulli Bai): فروش زنان به صورت مصنوعی در هند}
\addcontentsline{toc}{section}{بولی بای \textenglish{\textbf{(Bulli Bai)}}: فروش زنان به صورت مصنوعی در هند}{\protect\numberline{}}
\noindent در جای دیگر، ما \textenglish{\textbf{deepfake}} ها را به‌عنوان شکل مخصوصاً موذیانه‌ای از تبلیغات محاسباتی توصیف کرده‌ایم، که عمدتاً به دلیل پتانسیل آن‌ها در دامن زدن به تنش بین کشورها، به خطر انداختن امنیت ملی و تضعیف سیاست خارجی و دیپلماسی بین‌المللی است. رویدادهای اخیر در هند نشان می دهد که چگونه می توان از رسانه های مصنوعی برای تعمیق خصومت های قومی و مذهبی (در این مورد با هدایت خشونت جنسی با انگیزه‌های سیاسی علیه زنان مسلمان) استفاده کرد.
}

«رعنا ایوب» یک روزنامه نگار مشهور تحقیقی در هند است.
او نه تنها یک زن هندی است که در حال مذاکره در مورد زندگی عمومی در یک کشور محافظه کار اجتماعی است، بلکه یکی از اعضای اقلیت مسلمان نیز است (و یکی که به دلیل انتقاد از اعضای حزب حاکم بهاراتیا جاناتا \textenglish{\textbf{(BJP)}} شهرت پیدا کرده است).
او در آوریل 2018 گزارشی جنجالی درباره تجاوز جنسی به دختری 8 ساله در کشمیر نوشت.
رعنا ایوب حتی به \textenglish{\textbf{BBC}} رفت و اعضای حزب ملی‌گرا \textenglish{\textbf{BJP}} را به راهپیمایی در حمایت از متهم دعوت کرد.
ضربه برگشتی به ایوب سریع اما غیرقابل پیش‌بینی بود.
افراد ناشناس شروع به پخش پیام‌های جعلی در توییتر کردند که ادعا می‌شد از طرف ایوب آمده است و نظراتی مانند «من از هند متنفرم»، «من از هندی‌ها متنفرم»، «من عاشق پاکستان هستم»، و «من عاشق متجاوزین به کودکان هستم و اگر آن‌ها این کار را به نام اسلام انجام می‌دهند، من از آن‌ها حمایت می‌کنم».

اما بدتر از آن هنوز در راه بود!
شخصی از داخل \textenglish{\textbf{BJP}} به ایوب هشدار داد که ویدیویی در \textenglish{\textbf{WhatsApp}} به اشتراک گذاشته شده‌است که دیدن آن برای او بسیار دشوار است.
آن‌ها به او گفتند:"من آن را برای تو می‌فرستم، اما به من قول بده که ناراحت نخواهی شد".
چیزی که ایوب دریافت کرد یک ویدیوی مستهجن بود که در آن صورت او بر روی بدن برهنه یک زن (بسیار جوان) نقش بسته بود!
او می‌گوید که این ویدیو در «تقریباً همه تلفن‌های هند» پخش شد.
شما می‌توانید خود را روزنامه‌نگار خطاب کنید، می‌توانید خود را یک فمینیست بنامید، اما در آن لحظه، من نمی‌توانستم این تحقیر را ببینم.
ایوب در پایان گفت: حتی با وجود اینکه هیچ کس فکر نمی‌کرد (یا قرار بود فکر کند) این پورنوگرافی جعلی واقعی است، اما تأثیر مورد‌نظر خود را داشت: "من از روی ناچاری کمی خودسانسور شده ام".

این تنها یکی از بسیاری از حوادث مشابه در هند است.
در پایان سال 2021، چندین زن (همگی مسلمان مانند ایوب) در یک سایت حراجی جعلی به نام «بولی بای» ظاهر شدند که یک موقعیت فوق‌العاده تحقیرآمیز برای زنان مسلمان است.
این زنان نیز در زندگی عمومی هند برجسته بودند: روزنامه نگاران، فعالان، و وکلا.
این سایت آن‌ها را در شرایط توهین آمیز، اغلب جنسی صریح یا تحقیرآمیز به تصویر کشیده است.
این تصاویر از حساب‌های رسانه‌های اجتماعی گرفته‌شده و سپس دستکاری شده‌اند تا زنان را در موقعیت‌های زننده به تصویر بکشند.

حدود 6 ماه قبل، سایت مشابهی به نام \textenglish{\textbf{"Sulli Deals"}} در فضای مجازی منتشر شده‌بود.
همانند \textenglish{\textbf{Bulli Bai}}، این تصاویر با تعریف \textenglish{\textbf{deepfake}} مطابقت ندارند.
آنها \textenglish{\textbf{«shallow fakes»}} بودند (رسانه های مصنوعی که قرار نیست باورشان شود، اما با این وجود، تأثیرات آنها بر اهدافشان به شدت محسوس است).
یکی از زنانی که در سایت های حراج جعلی هدف قرار گرفته، دانشجوی ۲۶ ساله دانشگاه کلمبیا به نام «هیبا بگ» است.
بگ نیز مانند ایوب از حزب حاکم و سیاست ملی گرایانه آن انتقاد کرده است.
او اظهار داشت که این "ارعاب با هدف مجبور کردن زنان مسلمانی است که صدای خود را علیه بی‌عدالتی بلند می‌کنند تا از زندگی عمومی کناره گیری کنند.
اما شما عقب نشینی نمی‌کنید، حتی اگر همه چیز طاقت‌فرسا شود".

«عصمت آرا»، یکی دیگر از قربانیان «بولی بای»، زمانی که در فهرست «بولی بای روز» قرار گرفت، تصویری از «فروخته شدن» خود در حراج منتشر کرد و خاطرنشان کرد که سال جدید 2022 او با «حس ترس» و انزجار آعاز شده‌است".
در مورد او نیز، این سایت از «تصویر اصلاح‌شده من در زمینه‌ای نامناسب، غیرقابل قبول و آشکارا زننده» استفاده کرده‌است.

یکی دیگر از قربانیان سایت‌های حراج وحشی، «قراتولین رهبر» (روزنامه‌نگار اهل کشمیر تحت مدیریت هند و همسر یک قاضی دادگاه عالی در دهلی است).
رهبر اظهار داشت: وقتی عکسم را دیدم گلویم سنگین شد، بازوهایم شل شده بود و بی حس شده بودم.
تکان دهنده و تحقیرکننده است.
رهبر اظهار داشت که سایت حراج جعلی "برای تحقیر زنان مسلمان" بسیار‌خوب بوده است.

سایت‌های حراج‌جعلی «حسیبه امین» را که به عنوان هماهنگ کننده‌ی رسانه‌های اجتماعی برای حزب مخالف کنگره کار می‌کند نیز هدف قرار داده‌اند.
او نگران است که استفاده از این سایت‌ها برای ترویج خشونت و تهدید علیه زنان اقلیت عواقبی داشته‌باشد که فراتر از توانایی آن‌ها برای تحقیر و سانسور زنان برجسته هندی است.
او می‌ترسد که تهدید به مرگ و ارعاب آنلاین به خشونت جنسی در دنیای واقعی دامن بزند.
او می‌پرسد: «ما چه تضمینی از سوی دولت داریم که فردا تهدید و ارعاب آنلاین به خشونت جنسی واقعی در خیابان‌ها تبدیل نشود؟».

