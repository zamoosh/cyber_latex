%! Author = zamoosh
%! Date = 6/10/23


\chapter{رسانه‌های‌مصنوعی و خشونت سیاسی}
\label{ch:رسانه‌های‌مصنوعی و خشونت سیاسی}
\phantomsection

\begin{quote}
    بنابراین رسانه‌های ترکیبی ظرفیت تخریب سرمایه‌ی اجتماعی و اعتبار را در پایه خود دارند، و این به همان اندازه صادق است که آیا محتوای آن‌ها را درست می‌پذیریم یا نه.
    مشکل اساسی‌تر این است که ما هیچ معیار عینی‌ای برای تعیین اینکه چه چیزی شایسته باور است نداریم.
    علاوه بر این، جلب توجه به پدیده رسانه‌های‌مصنوعی تنها تأثیرات آن را تقویت می‌کند و برای ما چاره‌ای جز بازگشت به ترجیحات، تعصبات و ایدئولوژی‌های سیاسی خود باقی نمی‌گذارد.
    \\\\
    \textbf{تریسی داودزول و شان گولتز \textenglish{\textbf{(Tracey Dowdeswell and Sean Goltz)}}}
    \newline
\end{quote}


{\setstretch{0.5}
\phantomsection
\section*{کودتا در گابن}
\label{sec:کودتا در گابن}
\addcontentsline{toc}{section}{کودتا در گابن}{\protect\numberline{}}
«گابن»، کشوری نسبتاً باثبات در سواحل غربی آفریقا است، در روزهای ابتدایی سال 2019 توسط یک کودتا لرزید. جرقه‌ی این کودتا تا حد زیادی توسط یک سخنرانی معمولی در شب سال‌نو توسط «رئیس جمهور علی بونگو اودیمبا» آغاز شد که در شبکه‌های اجتماعی منتشر شده‌بود. رئیس جمهور مدتی بود که از انظار عمومی دور بود (در واقع او بیش از دو ماه بود که در کشور نبود) و شایعاتی مبنی بر اینکه او به شدت بیمار است یا حتی مرده است، در شبکه‌های اجتماعی پیچیده بود.
}

%   \textenglish{\textbf{}}


پیام سال‌نو به طور گسترده‌ای به عنوان یک «\textenglish{\textbf{deepfake}}» محکوم شد.
در حالی که پیام سال نو به خودی خود غیرقابل توجه بود، نحوه نمایش علی بونگو در این ویدئو کاملاً عجیب است: چهره رئیس جمهور بالای دهانش به طرز عجیبی بی حرکت است.
او به مدت 1 دقیقه و 39 ثانیه پس از فیلم، چشمانش اصلاً پلک نمی‌خورند و گفتار و حرکات او غیرطبیعی و مصنوعی به نظر می‌رسد.
نظرات به این ویدیو نشان می‌دهد که بینندگان احساس می‌کردند که این ویدیو «وحشتناک» و «\textenglish{\textbf{deepfake}}» است.
در 3 ژانویه 2019، یک رسانه‌ی خبری برجسته در گابن، مقاله‌ای را منتشر کرد که آشکارا این ویدئو را یک \textenglish{\textbf{deepfake}} محکوم کرد.

"رسانه مصنوعی" اصطلاحی فنی است که برای توصیف آنچه اغلب "\textenglish{\textbf{deepfake}}" نامیده می‌شود استفاده می‌شود.
رسانه‌های مصنوعی از یادگیری ماشین و شبکه‌های عصبی برای ایجاد رسانه‌های صوتی، عکس، ویدیو یا حتی متن مصنوعی استفاده می‌کنند که معتبر به نظر می‌رسند.
فیلم ویدئویی را می‌توان تغییر داد تا گفتار و حرکات یک نفر را بر روی صحبت‌های شخص دیگر قرار دهد.
رسانه‌های مصنوعی فراتر از فیلم‌های عکس و ویدیو هستند و شامل گفتار و نوشتار تولید شده توسط هوش‌مصنوعی می‌شوند.
این روش‌ها برای تولید حجم فزاینده‌ای از محتوایی که در اینترنت می‌بینیم استفاده می‌شوند و توسط «مزدوران» برای تولید ایمیل‌ها، متن‌ها یا پیام‌های نادرست که به نظر می‌رسد از سوی افرادی هستند که شما را به خوبی می‌شناسند، استفاده می‌شوند.
متن مصنوعیِ ساخته‌شده توسط هوش‌مصنوعی، حتی برای تولید مقالات آکادمیک نیز استفاده شده است؛ و جالب است که بدایند آن مقاله به چاپ نیز رسیده‌است!
چندین ادعا در مجله \textenglish{\textbf{\mbox{Arabian Journal of Geosciences}}} به دلیل بسیار غیرمعمولی، به 10 ادعای برتر ایوان اورانسکی در سال 2021 تبدیل شد؛ آن‌ها چرندیات محض بودند!
[توضیح ادعای اورانسکی: Retraction Watch، وبلاگی است که در مورد ادعای مقالات علمی و موضوعات مرتبط گزارش می‌دهد.
این وبلاگ در آگوست 2010 راه‌اندازی شد و توسط نویسندگان علم، ایوان اورانسکی (معاون سابق، \textenglish{\textbf{\mbox{Editorial Medscape}}}) و آدام مارکوس (ویرایشگر اخبار گوارش و اندوسکوپی) تولید می‌شود].
یک مقاله باید از مجله \textenglish{\textbf{Arabian Journal of Geosciences}} به این دلیل غیرعادی که «محتوای این مقاله مزخرف است» پس گرفته شود.
به نظر می‌رسد این مشکل بسیار گسترده شده است: بیش از 400 مقاله در مجلات متعلق به \textenglish{\textbf{Springer Nature}} و صدها مقاله‌ی دیگر در \textenglish{\textbf{Elsevier}} نیز آلوده به متن مصنوعی هستند!
فناوری‌های رسانه‌ی‌مصنوعی سریع‌تر از فناوری‌هایی که برای شناسایی آن‌ها استفاده می‌شوند توسعه می‌یابند و \textenglish{\textbf{deepfake}} را تقریباً نامرئی می‌کنند.

نیروهای مسلح گابن (که مدت‌ها در مخالفت با حزب حاکم بونگو بودند) با ویدیوی سال نو رئیس جمهور نیز احساس کردند که به تمسخر گرفته شده‌اند.
در ساعات اولیه صبح 7 ژانویه 2019، «ستوان کِلی اوندو اوبیانگ» از گارد جمهوری خواه اعلام کرد که سخنرانی سال نو نشان داد که «علی بونگو» برای اداره کشور مناسب نیست.
بر این اساس، ارتش او را از سمت خود برکنار کرد و می‌خواست «شورای بازسازی» ملی را برای حکومت به جای او تشکیل دهد.
تانک‌ها وارد لیبرویل (پایتخت) شدند.
اینترنت و برق قطع شد.
حدود 300 معترض به حمایت از کودتا آمدند و توسط نیروهای دولتی با گاز اشک آور مورد حمله قرار گرفتند.
صدای تیراندازی در پایتخت شنیده شد.
تا ساعت 10 صبح، نیروهای دولتی دوباره کنترل شدند و رهبران کودتا یا مرده بودند یا در بازداشت بودند.

کارشناسان هوش‌مصنوعی شروع به آزمایش کردند که آیا ویدیوی سال نو ساخته‌شده‌است یا خیر، و به سرعت یک اجماع واضح ظاهر شد.\ «استیو گروبمن (\textenglish{\textbf{Steve Grobman}})» (مدیر ارشد فناوری \textenglish{\textbf{McAfee}}) ویدیو را از طریق الگوریتم‌های آن‌ها اجرا کرد و با احتمال بسیار بالا (92٪) تشخیص داد که ویدیو واقعی است.
سیوی لیو (\textenglish{\textbf{Siwei Lyu}})، پروفسور علوم کامپیوتر در «\textenglish{\textbf{\mbox{SUNY Albany}}}»، همچنین ویدیو را با استفاده از الگوریتم \textenglish{\textbf{deepfake}} خود بررسی کرد و تائید کرد که به احتمال 99٪ ویدئو واقعی است.
الگوریتم‌ها، هیچ مدرکی مبنی بر استفاده از روش‌های شناخته‌شده‌ی مصنوعی‌کردن رسانه، برای تولید ویدئو پیدا نکردند.
اگر این یک \textenglish{\textbf{deepfake}} بود، باید یک فرضیه بسیار پیچیده بوده باشد (فرضیه‌ای که توسط عجیب بودن خود ویدیو پشتیبانی نمی‌شود).

صحت این ویدیو با مشاهده خود علی بونگو در حضورهای عمومی بعدی تأیید‌شد که نشان می‌دهد او زنده است (اما همچنین بسیار تغییر یافته است).
«الکساندر درومریک» (\textenglish{\textbf{Alexander Dromeric}})، متخصص مغز و اعصاب واشینگتن پست، اظهار داشت که حرکات و بی‌تحرکی صورت علی بونگو مشخصه افرادی است که سکته کرده یا دچار نوعی آسیب مغزی شده‌اند.
از آن زمان دفتر رسمی مطبوعاتی دولت از تایید یا تکذیب اینکه آیا علی بونگو دچار سکته مغزی شده است خودداری کرده است.
دولت گابن در واقع به اطلاعات نادرست و بی اعتمادی در مورد سلامت رئیس جمهور دامن می زد (اما به دلیل عدم شفافیت آن‌ها و نه با تولید رسانه‌های مصنوعی).
مشکل \textenglish{\textbf{deepfake}} فقط این نیست که اطلاعات نادرست تولید می‌کنند، بلکه وجود آن‌ها مردم را به سمت بی اعتبار کردن گزارش‌هایی سوق می‌دهد که در واقع درست هستند.

«آویو اوادیا» (\textenglish{\textbf{Aviv Ovadya}})، متخصص هوش‌مصنوعی و رسانه‌های مصنوعی، بیان می‌کند که \textenglish{\textbf{deepfake}} می‌تواند بسیار خطرناک باشد، دقیقاً به این دلیل که بی اعتمادی گسترده نسبت به همه‌ی رسانه‌ها ایجاد می‌کند.
این باعث ایجاد نوعی «بی‌تفاوتی واقعیت» می‌شود که اوادیا آن را «\textenglish{\textbf{Infocalypse}}» نامیده است (از دست دادن اساسی اعتماد به نهادهای اجتماعی).
یک \textenglish{\textbf{Infocalypse}} زمانی به وجود می‌آید که متوجه می‌شویم استانداردهایی برای حقیقت و عینیت نداریم و کنترلی بر فناوری‌های در حال تکامل سریع نداریم، همانطور که در گابن اتفاق افتاد.
اوادیا می‌گوید که «مخاطره‌ها زیاد است و پیامدهای احتمالی فاجعه‌بارتر از مداخله خارجی در انتخابات است («\textenglish{\textbf{Infocalypse}}»: تضعیف یا فروپاشی نهادهای اصلی).

بی‌تفاوتی واقعی که توسط رسانه‌های‌مصنوعی ایجاد می‌شود پرهزینه است (همانطور که اوادیا می‌گوید، «هم برای سازمان‌های رسانه‌ای که مجبور به صرف زمان و منابع برای بررسی و شناسایی جعلی بودن این شکل ویدیوها هستند هم برای جوامعی که در بحث‌هایی درباره اصالت آن‌ها صحبت می‌کنند»).
برای گابن، این هزینه‌ها در خشونت سیاسی، افزایش ترس، گسست اجتماعی و مرگ دو عضو نیروهای مسلح شورشی دیده‌شد.
\newline
\newline



{\setstretch{0.5}
\phantomsection
{\setstretch{1}
\section*{بولی بای \textenglish{\textbf{(Bulli Bai)}}: فروش زنان به صورت مصنوعی در هند}
}
\label{sec:بولی بای (Bulli Bai): فروش زنان به صورت مصنوعی در هند}
\addcontentsline{toc}{section}{بولی بای \textenglish{\textbf{(Bulli Bai)}}: فروش زنان به صورت مصنوعی در هند}{\protect\numberline{}}
\noindent در جای دیگر، ما \textenglish{\textbf{deepfake}}‌ها را به‌عنوان شکل مخصوصاً موذیانه‌ای از تبلیغات محاسباتی توصیف کرده‌ایم، که عمدتاً به دلیل پتانسیل آن‌ها در دامن زدن به تنش بین کشورها، به خطر انداختن امنیت ملی و تضعیف سیاست خارجی و دیپلماسی بین‌المللی است. رویدادهای اخیر در هند نشان می دهد که چگونه می توان از رسانه‌های مصنوعی برای تعمیق خصومت‌های قومی و مذهبی (در این مورد با هدایت خشونت جنسی با انگیزه‌های سیاسی علیه زنان مسلمان) استفاده کرد.
}

«رعنا ایوب» یک روزنامه نگار مشهور تحقیقی در هند است.
او نه تنها یک زن هندی است که در حال مذاکره در مورد زندگی عمومی در یک کشور محافظه کار اجتماعی است، بلکه یکی از اعضای اقلیت مسلمان نیز است (و یکی که به دلیل انتقاد از اعضای حزب حاکم بهاراتیا جاناتا \textenglish{\textbf{(BJP)}} شهرت پیدا کرده است).
او در آوریل 2018 گزارشی جنجالی درباره تجاوز جنسی به دختری 8 ساله در کشمیر نوشت.
رعنا ایوب حتی به \textenglish{\textbf{BBC}} رفت و اعضای حزب ملی‌گرا \textenglish{\textbf{BJP}} را به راهپیمایی در حمایت از متهم دعوت کرد.
ضربه برگشتی به ایوب سریع اما غیرقابل پیش‌بینی بود.
افراد ناشناس شروع به پخش پیام‌های جعلی در توییتر کردند که ادعا می‌شد از طرف ایوب آمده است و نظراتی مانند «من از هند متنفرم»، «من از هندی‌ها متنفرم»، «من عاشق پاکستان هستم»، و «من عاشق متجاوزین به کودکان هستم و اگر آن‌ها این کار را به نام اسلام انجام می‌دهند، من از آن‌ها حمایت می‌کنم».

اما بدتر از آن هنوز در راه بود!
شخصی از داخل \textenglish{\textbf{BJP}} به ایوب هشدار داد که ویدیویی در \textenglish{\textbf{WhatsApp}} به اشتراک گذاشته شده‌است که دیدن آن برای او بسیار دشوار است.
آن‌ها به او گفتند:"من آن را برای تو می‌فرستم، اما به من قول بده که ناراحت نخواهی شد".
چیزی که ایوب دریافت کرد یک ویدیوی مستهجن بود که در آن صورت او بر روی بدن برهنه یک زن (بسیار جوان) نقش بسته بود!
او می‌گوید که این ویدیو در «تقریباً همه تلفن‌های هند» پخش شد.
شما می‌توانید خود را روزنامه‌نگار خطاب کنید، می‌توانید خود را یک فمینیست بنامید، اما در آن لحظه، من نمی‌توانستم این تحقیر را ببینم.
ایوب در پایان گفت: حتی با وجود اینکه هیچ کس فکر نمی‌کرد (یا قرار بود فکر کند) این پورنوگرافی جعلی واقعی است، اما تأثیر مورد‌نظر خود را داشت: "من از روی ناچاری کمی خودسانسور شده ام".

این تنها یکی از بسیاری از حوادث مشابه در هند است.
در پایان سال 2021، چندین زن (همگی مسلمان مانند ایوب) در یک سایت حراجی جعلی به نام «بولی بای» ظاهر شدند که یک موقعیت فوق‌العاده تحقیرآمیز برای زنان مسلمان است.
این زنان نیز در زندگی عمومی هند برجسته بودند: روزنامه نگاران، فعالان، و وکلا.
این سایت آن‌ها را در شرایط توهین آمیز، اغلب جنسی صریح یا تحقیرآمیز به تصویر کشیده است.
این تصاویر از حساب‌های رسانه‌های اجتماعی گرفته‌شده و سپس دستکاری شده‌اند تا زنان را در موقعیت‌های زننده به تصویر بکشند.

حدود 6 ماه قبل، سایت مشابهی به نام \textenglish{\textbf{"Sulli Deals"}} در فضای مجازی منتشر شده‌بود.
همانند \textenglish{\textbf{Bulli Bai}}، این تصاویر با تعریف \textenglish{\textbf{deepfake}} مطابقت ندارند.
آن‌ها \textenglish{\textbf{«shallow fakes»}} بودند (رسانه‌های مصنوعی که قرار نیست باورشان شود، اما با این وجود، تأثیرات آن‌ها بر اهدافشان به شدت محسوس است).
یکی از زنانی که در سایت‌های حراج جعلی هدف قرار گرفته، دانشجوی ۲۶ ساله دانشگاه کلمبیا به نام «هیبا بگ» است.
بگ نیز مانند ایوب از حزب حاکم و سیاست ملی گرایانه آن انتقاد کرده است.
او اظهار داشت که این "ارعاب با هدف مجبور کردن زنان مسلمانی است که صدای خود را علیه بی‌عدالتی بلند می‌کنند تا از زندگی عمومی کناره گیری کنند.
اما شما عقب نشینی نمی‌کنید، حتی اگر همه چیز طاقت‌فرسا شود".

«عصمت آرا»، یکی دیگر از قربانیان «بولی بای»، زمانی که در فهرست «بولی بای روز» قرار گرفت، تصویری از «فروخته شدن» خود در حراج منتشر کرد و خاطرنشان کرد که سال جدید 2022 او با «حس ترس» و انزجار آعاز شده‌است".
در مورد او نیز، این سایت از «تصویر اصلاح‌شده من در زمینه‌ای نامناسب، غیرقابل قبول و آشکارا زننده» استفاده کرده‌است.

یکی دیگر از قربانیان سایت‌های حراج وحشی، «قراتولین رهبر» (روزنامه‌نگار اهل کشمیر تحت مدیریت هند و همسر یک قاضی دادگاه عالی در دهلی است).
رهبر اظهار داشت: وقتی عکسم را دیدم گلویم سنگین شد، بازوهایم شل شده بود و بی حس شده بودم.
تکان دهنده و تحقیرکننده است.
رهبر اظهار داشت که سایت حراج جعلی "برای تحقیر زنان مسلمان" بسیار‌خوب بوده است.

سایت‌های حراج‌جعلی «حسیبه امین» را که به عنوان هماهنگ کننده‌ی رسانه‌های اجتماعی برای حزب مخالف کنگره کار می‌کند نیز هدف قرار داده‌اند.
او نگران است که استفاده از این سایت‌ها برای ترویج خشونت و تهدید علیه زنان اقلیت عواقبی داشته‌باشد که فراتر از توانایی آن‌ها برای تحقیر و سانسور زنان برجسته هندی است.
او می‌ترسد که تهدید به مرگ و ارعاب آنلاین به خشونت جنسی در دنیای واقعی دامن بزند.
او می‌پرسد: «ما چه تضمینی از سوی دولت داریم که فردا تهدید و ارعاب آنلاین به خشونت جنسی واقعی در خیابان‌ها تبدیل نشود؟».
\newline
\newline
\newline


{\setstretch{0.5}
\phantomsection
\section*{تفسیر}
\label{sec:تفسیر}
\addcontentsline{toc}{section}{تفسیر}{\protect\numberline{}}

\phantomsection
\subsection*{اخلاق فضیلت}
\label{subsec:اخلاق فضیلت}
\addcontentsline{toc}{subsection}{اخلاق فضیلت}{\protect\numberline{}}
\noindent \textbf{توسط پیتر سینگر و ییپ فای تسه}
\newline
\newline

\phantomsection
\subsubsection*{ملاحظات کلیدی - ارزش حقیقت}
\label{subsubsec:ملاحظات کلیدی - ارزش حقیقت}
\addcontentsline{toc}{subsubsection}{ملاحظات کلیدی - ارزش حقیقت}{\protect\numberline{}}
ما برای حقیقت ارزش زیادی قائل هستیم و نادیده‌گرفتن این ارزش را بسیار دشوار می‌دانیم. بر این اساس، ما همچنین به شیوه‌هایی که حقیقت و صداقت فکری را در بر می‌گیرند، مانند توسعه روش‌های علمی که به شواهد و به روز کردن باورها در پرتو بهترین شواهد موجود بستگی دارد، ارزش می‌گذاریم. در مقابل، وقتی حقیقت را نمی‌پذیریم، برای مثال، اگر جامعه استفاده نادرست زیادی از فناوری‌های \textenglish{\textbf{deepfake}} مشاهده کند، احتمالاً بی اعتمادی عمومی نسبت به صحت صدا، فیلم و تصاویر افزایش می‌یابد. این در مورد «کودتای گابن» نشان داده شده‌است. و نه تنها بر سیاست‌های محلی تأثیر می‌گذارد. این نگران‌کننده است که با فناوری‌های \textenglish{\textbf{deepfake}} که برای فریب دادن یا گمراه کردن افراد و سهولت دسترسی به آن‌ها طراحی شده‌اند، ممکن است نقطه‌ای وجود داشته‌باشد که هرکسی بتواند هر کاری را در هر مکان (یا در مکان‌های خیالی) انجام دهد. عواقب آن ممکن است شامل استفاده از هر فیلم یا عکسی به عنوان مدرک در دادگاه باشد، اما محدود به آن نیست. دولت‌ها و سازمان‌های غیردولتی قادر به شناسایی موارد نقض حقوق بشر نیستند. و با تغییر تفکر، کسانی که مرتکب جنایت می‌شوند از محکومیت فرار می‌کنند، زیرا می‌توانند صحت مدارک علیه خود را به طور قابل‌قبولی انکار کنند.
\newline
\newline
}


{\setstretch{0.5}
\phantomsection
\subsubsection*{آیا کل قضیه‌ی \textenglish{\textbf{deepfake}} همین است؟}
\label{subsubsec:آیا کل قضیه‌ی deepfake همین است؟}
\addcontentsline{toc}{subsubsection}{آیا کل قضیه‌ی \textenglish{\textbf{deepfake}} همین است؟}{\protect\numberline{}}
برای برخی، ممکن است به نظر برسد که \textenglish{\textbf{deepfake}} یک مشکل کاملاً جدید است، اما اینطور نیست. «نانسی پلوسی»، رئیس مجلس نمایندگان ایالات متحده، هدف چند ویدیوی تغییر‌یافته بود تا صدای او را مبهم یا مست کند، از جمله سخنرانی او روی صحنه در یک رویداد مرکز پیشرفت آمریکا. آن ویدئوها به سرعت پخش شد و به عنوان مدرکی علیه شایستگی و اخلاق کاری او استفاده شد. مشخص شد که سرعت فیلم‌ها فقط تا \textenglish{\textbf{0.75}} کاهش یافته است، عملکردی که در طیف گسترده‌ای از نرم‌افزارهای اصلی پخش یا ویرایش ویدیو موجود است. از این رو، ما نباید وسوسه شویم که فکر کنیم فناوری‌های \textenglish{\textbf{deepfake}} تمام مسئولیت را بر عهده می‌گیرند. همچنین نباید فکر کنیم که \textenglish{\textbf{deepfake}} "چیزی بیش از فتوشاپ" نیست. استفاده از نرم‌افزارهایی مانند فتوشاپ به آموزش و تجربه، شاید استعداد نیز نیاز دارد، اما فناوری‌های \textenglish{\textbf{deepfake}} به رابط‌های برنامه‌نویسی کاربردی (بیش از حد آسان) یا API تبدیل شده‌اند که این امکان را برای میلیون‌ها نفر فراهم می‌کند که با چند کلیک \textenglish{\textbf{deepfake}} بسازند. همچنین، همانطور که نتایج کنونی نشان داده‌است، فناوری‌های \textenglish{\textbf{deepfake}} مسلماً قوی‌تر از هر روش قبلی‌ای برای مصنوعی‌سازی رسانه‌ها هستند. و مهم‌تر از همه، آن‌ها پیچیده‌تر خواهند‌شد و تشخیص آن‌ها سخت‌تر می‌شود، زیرا الگوریتم‌های پشت آن‌ها می‌توانند به طور مداوم با تحقیقات بیشتر و آموزش بیشتر با داده‌ها بهبود یابند.
}

شاید بتوان به فناوری‌های تشخیص عمقی به عنوان دلیلی اشاره کرد که چرا نباید زیاد نگران باشیم.
اما آن‌ها نیز مشکلاتی دارند.
اول، ممکن است \textenglish{\textbf{100\%}} قابل اعتماد نباشند.
دوم، حتی اگر آن‌ها بتوانند \textenglish{\textbf{deepfake}} ها را از رسانه‌های واقعی با اطمینان بالا شناسایی کنند، تشخیص را از دست افرادی که نمی‌توانند به نرم‌افزار تشخیص دسترسی داشته‌باشند، رها می‌کند.
سوم، ممکن است ما را به رد شواهد معتبر به دلیل استفاده نادرست گسترده از \textenglish{\textbf{deepfake}} ‌ها و بی‌اعتمادی که این امر ایجاد می‌کند، سوق دهد.
چهارم، گاهی اوقات تشخیص نمی‌تواند آسیبی را که قبلاً وارد شده است، از بین ببرد، مثلاً در مورد پورنوگرافی بدون رضایت.
پنجم، روش‌های تشخیص ممکن است خود به بهبود فناوری‌های \textenglish{\textbf{deepfake}} کمک کنند، یا با وادار کردن طراحان یا الگوریتم‌های \textenglish{\textbf{deepfake}} به بهتر شدن، یا حتی به‌طور مستقیم برای تبدیل شدن به ابزاری برای آموزش این الگوریتم‌ها (مثلاً به‌عنوان متمایزکننده شبکه‌های آموزشی متخاصم مولد).

برخی ممکن است این نکته را مطرح‌کنند که فایده‌گرایی استفاده از \textenglish{\textbf{deepfake}} را توجیه می‌کند، حتی اگر به برخی افراد آسیب وارد کند، تا زمانی که افراد بیشتری از آن سود ببرند.
مثالی که در آن مردم (به ظاهر سودمند‌گرا نیستند) چنین استدلالی را مطرح کرده‌اند، پورنوگرافی مصنوعی بدون رضایت است.
ما اذعان می‌کنیم که تعداد افرادی که از چنین موادی لذت می‌برند بسیار بیشتر از تعداد قربانیان است.
اما این استدلال، سخنی مضحک از آن چیزی است که سودگرایان ادعا می‌کنند، هر چند که شاید با شعار گمراه‌کننده «بزرگترین خوشبختی از بیشترین تعداد» ایجاد شده‌باشد.
درست است که «جرمی بنتام»، بنیانگذار فایده‌گرایی، از این شعار استفاده کرد، اما بعداً وقتی متوجه شد که این شعار به این معناست که هر چیزی که به نفع 51 درصد جمعیت باشد، درست است، آن را رد کرد، حتی اگر 51 درصد از آن سود ببرند.
فقط اندکی و 49 درصد آسیب‌های بزرگی را متحمل می‌شوند.
در موردی که در اینجا مورد بحث قرار‌گرفت، اگر پورنوگرافی مصنوعی ساخته‌شود، زنانی که توسط آن به تصویر کشیده می‌شوند آسیب‌هایی را متحمل می‌شوند که از نوع کاملاً متفاوت، و بسیار جدی‌تر از دست دادن «منافع» ناتوانی در مشاهده چنین مواردی است.

علاوه بر ارزش بلندمدت حقیقت که قبلاً به آن اشاره کردیم، پیامدهای مهم و بلندمدت دیگری نیز از پورنوگرافی مصنوعی غیرقانونی وجود دارد: این ایده را تقویت و تداوم می‌بخشد که زنان، اشیایی هستند که ممکن است، بدون رضایت آن‌ها، مورد استفاده قرار‌گیرند.
لذت بردن از دیگران، و این باعث ترویج و ایجاد نگرش‌های قابل‌قبول تر می‌شود که برای زنان در بسیاری از جنبه های مختلف زندگی مضر است.
\newline
\newline


{\setstretch{0.5}
\phantomsection
\subsection*{اخلاق آفریقایی}
\label{subsec:اخلاق آفریقایی}
\addcontentsline{toc}{subsection}{اخلاق آفریقایی}{\protect\numberline{}}
\noindent \textbf{نوشته جان مورانگی}
\\\\
فکر کردن به جایگاه اخلاق در جهان که توسط علم داده یا، به طور خاص، توسط هوش مصنوعی ایجاد شده‌است، دشوار است. من گمان می کنم که اکثر دانشمندان داده و افراد هوش مصنوعی باور ندارند که در کاری که انجام می‌دهند یک جهانی خلق می‌کنند یا ساکنان چنین دنیایی هستند. بسیاری از آن‌ها ممکن است از این دنیا بی‌خبر باشند و معمار آن باشند. همچنین ممکن است برخی از آن‌ها بدانند که معماران آن هستند و در آن زندگی می‌کنند. اما حتی اگر آن‌ها چنین دانشی داشته‌باشند، ممکن است که درک کاملی از پیامدهای آن نداشته‌باشند.
}

به نظر من آنچه برای تعیین معنادار جایگاه اخلاق در جهان ایجاد شده توسط علم داده و هوش‌مصنوعی لازم است، تعمیق و گسترش آگاهی از پیامدهای آن است.
در طی انجام این کار، آگاهی از وجود دیدگاهی برخاسته از دنیایی متفاوت، جهانی که عمدتاً اخلاقی است، می‌تواند به عنوان پادزهری برای جنبه‌های غیرانسانی جهان که توسط علم داده و هوش‌مصنوعی ایجاد می‌شود، عمل کند که در چارچوب چنین دیدگاهی است که من به پیوند بین رسانه‌های مصنوعی و خشونت سیاسی فکر می‌کنم.

در مورد گابن، رئیس جمهور علی بونگو که به طور مصنوعی تولید شده است، به سختی می توان از رئیس جمهور بونگو که ساخته‌نشده تشخیص داد.
به طور مشابه، تشخیص زن روزنامه نگار هندی ساخته‌شده از روزنامه نگار زن هندی ساخته‌نشده دشوار است.
حتی اگر وسیله‌ای برای تشخیص یکی از دیگری وجود داشته باشد، خود وسیله نمی‌تواند آسیبی را که قبلاً وارد شده است جبران کند و هیچ تضمینی وجود ندارد که در آینده آسیب بیشتری ایجاد نشود.
دشواری تشخیص واقعیت از جعلی همچنان ما را آزار می‌دهد.

علاوه بر این، تأیید ممکن است در معرض دستکاری هوش‌مصنوعی باشد.
در مواقعی، مشروعیت تأیید بستگی به چیزی دارد که شخص می‌خواهد تأیید کند و آنچه که می‌خواهد تأیید کند می‌تواند ساخته‌اش باشد!
نباید قدرت متقاعدسازی رسانه‌های مصنوعی را دست‌کم گرفت.
هوش‌مصنوعی آینده‌ای دست‌نخورده دارد.
پیشرفت آن آینده قابل پیش‌بینی مشخصی ندارد.
قدرت بی وقفه‌ای برای خلق آینده‌ی خود دارد.
علاوه بر این، ممکن است آنچه در نظر بیننده واقعی است و همچنین مصنوعی باشد.

دنیای رسانه‌های‌مصنوعی می‌تواند در ظاهر واقعی‌تر از دنیای واقعی باشد.
برای برخی، به طور فزاینده‌ای در حال تبدیل شدن به دنیای‌واقعی است.
همچنین باید توجه داشت که اخلاق خود از دستکاری رسانه‌های‌مصنوعی مصون نیست.
خوبی که هدف آن است می‌تواند خیری باشد که توسط هوش‌مصنوعی تعیین می‌شود.
آسیبی که تصور می‌شود محصول هوش‌مصنوعی است، ممکن است به عنوان مخالف آن تلقی شود: به عنوان خوب.
می‌توان تصور کرد که کسانی که رئیس‌جمهور جعلی را در گابن تولید کردند، همانطور که در مورد کسانی که یک روزنامه‌نگار زن هندی جعلی تولید کردند، چیزی مضر تولید نمی‌کردند.
تا این حد، به نظر نمی‌رسد که لزوماً ارتباط علی بین هوش‌مصنوعی و خشونت سیاسی وجود داشته‌باشد.

پیوند بین رسانه‌های‌مصنوعی و خشونت سیاسی توجه را به پیوند بین علمِ داده و هوش‌مصنوعی در سیاست جلب می‌کند.
نه پیوند و نه آنچه که مستلزم آن است بدیهی است.
آموزش دانشمندان داده یا آموزش بازیگران هوش‌مصنوعی شامل مطالعه سیاست نمی‌شود.
ظاهراً غیرسیاسی یا از نظر سیاسی بی‌طرف به نظر می‌رسد.
حتی اگر مطالعه سیاست نیز گنجانده شود، به احتمال زیاد علم سیاست به معنای پوزیتیویستی آن خواهد بود.
به احتمال زیاد، مطابق با سایر علوم اجتماعی، به ظاهر یک علم سیاسی غیرسیاسی خواهد‌بود.

جوامع عمدتاً برای رفاه آن‌ها از نظر سیاسی تأسیس شده‌اند.
این رفاه در درجه اول موضوع اخلاق است.
اگر این را پذیرفت، نمی‌توان امر سیاسی را از امر اخلاقی و امر اخلاقی را از امر سیاسی جدا کرد.
امر اخلاقی امر سیاسی است و امر سیاسی امری اخلاقی است.
وقتی پوزیتیویسم مطالعه سیاست را تعیین می‌کند، علم سیاسی که چنین تعیین شده‌است جایی برای اخلاق ندارد.
در اینجا، امر سیاسی بدون احساس امر اخلاقی در علم داده غیرسیاسی و در غیرسیاسی در هوش مصنوعی جایگاهی دارد.
معرفی اخلاق در علم داده و در آموزش هوش‌مصنوعی به عنوان یک حواس پرتی در این آموزش ظاهر می‌شود.
این آموزش اغلب به عنوان آموزش بدون ارزش پیش بینی می‌شود (آموزش بدون اخلاق، به عنوان آموزش غیرسیاسی).
در جوامع بومی آفریقا، رفاه اجتماعی یک امر اشتراکی است.
این رفاه هم سیاسی و هم اخلاقی است.
این بهزیستی است که جایگاهی برای رفاه فردی و همچنین رفاه گروهی دارد.
در هر دو صورت، چنین رفاهی به بهای رفاه جامعه نیست.
این حس گسترده تر از رفاه اجتماعی بر مفهوم آفریقایی اوبونتو استوار است.
در حالت اوبونتو انسان، ادعا می‌شود که "ما هستیم، پس من هستم." به رسمیت شناخته شده‌است که در جستجوی رفاه، جایی برای یک فرد یا گروهی برای بهزیستی وجود دارد، اما نه به قیمت سعادت جامعه.
همچنین مشخص شده است که یک فرد یا یک گروه می‌تواند بر خلاف رفاه اجتماعی عمل کند، اما کنترل های اجتماعی برای به حداقل رساندن تهدید برای رفاه جامعه وجود دارد.
این کنترل‌ها به دلیل تغییرات مدرن در جامعه ضعیف شده‌است.

تغییرات جامعه از درون و بیرون، ساخت رفاه اجتماعی آفریقا را پیچیده کرده‌است.\ «ما» در اوبونتو پیچیده شده است.
دیگر نمی‌توان «ما» واقعی را از «ما» جعلی تشخیص‌داد.
هر چیزی که به عنوان «ما» واقعی درک می‌شد، توسط «ما» مصنوعی واژگون شده است.
تشخیص آسیب به جامعه به دلیل نقش پیچیده‌ای که هوش‌مصنوعی در دستکاری ادراک بازی می‌کند، دشوار شده‌است.
یکی از پیامدهای هوش‌مصنوعی، تجاوز به مرزهای اجتماعی است.
امروزه، هیچ جامعه‌ای در آفریقا یا هر جای دیگری از تجاوزات روزافزون هوش‌مصنوعی مصونیت ندارد.
تشخیص آنچه واقعی از جعلی است یا تشخیص بی‌ضرر از آنچه مضر است به طور فزاینده‌ای دشوار است.
پلیس در فضای مجازی درست است، اما چه کسی بر پلیس نظارت دارد؟ آیا بازیگران هوش‌مصنوعی می‌توانند به عنوان افسر‌پلیس خدمت کنند؟ چه کسی بر آن‌ها نظارت خواهد‌کرد؟ آن‌ها باید چه آموزش سیاسی/اخلاقی داشته‌باشند تا بتوانند این کار را در راستای تحقق عدالت اجتماعی انجام دهند؟ آیا خود عدالت اجتماعی تبدیل به یک محصول هوش‌مصنوعی نشده‌است؟ آیا ما در عصر استبداد هوش‌مصنوعی زندگی نمی‌کنیم؟

امروزه به نظر می‌رسد که بازیگران هوش‌مصنوعی کشیشان عصر‌ما هستند.
آن‌ها به عنوان الهیات سیاسی عمل می‌کنند.
آن‌ها در جامعه به عنوان ناجی نوع بشر شناخته می‌شوند.
مراکز هوش‌مصنوعی هاله‌ی تقدس را به خود گرفته‌اند.
آیا این مستلزم الحاد جدید نیست؟ کافران جدید؟
\newline
\newline



{\setstretch{0.5}
\phantomsection
\subsection*{اخلاق بومی}
\label{subsec:اخلاق بومی}
\addcontentsline{toc}{subsection}{اخلاق بومی}{\protect\numberline{}}
\noindent \textbf{توسط جوی میلر و آندریا سالیوان کلارک}
\\\\
سؤال اخلاقی اصلی که مایلیم به آن بپردازیم در پایان این مطالعه موردی مطرح شده‌است. آیا "هدف قرار دادن زنان فعال سیاسی در هند از طریق پورنوگرافی عمیق، "سخنرانی خطرناک" را تشکیل می‌دهد؟ در یک درک بومی از اخلاق، کاملا بله.
}

برای درک چرایی آن، باید حداقل دو دسته از مسائل اخلاقی را که در پاسخ به این سؤال مطرح می‌شوند، در نظر بگیریم.
اول، این موضوع وجود دارد که چه چیزی مورد استفاده قرار می‌گیرد.
در این مورد، استفاده از پورنوگرافی‌عمیق‌جعلی نگرانی‌های اخلاقی را در مورد چگونگی و چرایی ایجاد و به دست آوردن چنین تصاویری ایجاد می‌کند.
دوم، این موضوع وجود دارد که چه چیزی از استفاده از پورنوگرافی‌عمیق‌جعلی حاصل می‌شود.
در این صورت استفاده از چنین تصاویری منجر به اجبار، فریب و انقیاد زنان فعال سیاسی در هند می‌شود.
برای توضیح اینکه چرا \textenglish{\textbf{deepfake}} ها در این زمینه گفتار خطرناکی را تشکیل می‌دهند، به نوبه‌ی خود هر دوی این نگرانی‌ها را بیشتر توضیح خواهیم‌داد.
با توجه به موضوع اخلاقی چگونگی و چرایی خلق و به دست آمدن چنین تصاویری، ایجاد چنین تصاویری نقض حاکمیت و خودمختاری است.
همانطور که در مورد رسانه‌های مصنوعی و خشونت سیاسی مشخص است، داده‌ها را می‌توان به سلاح تبدیل کرد.
حتی اگر مصنوعی باشد، می توان از آن برای آسیب رساندن، کنترل، سرکوب و سلب حق رای دیگران استفاده کرد.
برای مردم بومی، تاریخچه‌ای از سلاح سازی داده‌ها وجود دارد.
به این ترتیب، جنبشی از سوی محققان بومی برای جمع‌آوری و کنترل داده‌های مربوط به مردمانشان وجود دارد.
جنبش حاکمیت داده‌ها شاهدی بر اهمیت حاکمیت و خودمختاری برای مردم بومی است.
این فقط به این نیست که چگونه داده‌ها را می‌توان تسلیحاتی کرد، بلکه این است که چه کسی بهتر می‌داند با داده‌ها چه کند.
مردم بومی در موقعیت بهتری برای استفاده (به عنوان مثال، جمع‌آوری و پیاده‌سازی) داده‌های مربوط به خود برای درک بهتر و زندگی خود نسبت به افراد خارجی که اغلب از چنین داده‌هایی برای برنامه‌های خود استفاده می‌کنند، هستند.

همین امر را می‌توان در مورد افرادی در هند نیز گفت که از شباهت هایشان برای ایجاد پورنوگرافی‌عمیق‌جعلی استفاده می‌شود.
در این مورد، هیچ رضایتی برای استفاده و ایجاد این موارد پورنوگرافی‌عمیق‌جعلی داده‌نشد.
داده‌ها (یعنی \textenglish{\textbf{deepfake}} ها) ناشی از استفاده از شباهت یک فرد بدون رضایت اوست.
این از نظر اخلاقی اشتباه است زیرا مقدار کافی حاکمیت (توانایی کنترل زندگی و تصمیم گیری شخصی) برای خوب زیستن ضروری است.
\textenglish{\textbf{deepfake}} ناشی از بی توجهی کامل به حاکمیت است.

برای درک این موضوع که چه چیزی از استفاده از پورنوگرافی‌عمیق‌جعلی حاصل می شود، یک ایده کلیدی از فلسفه‌ی بومی باید درک شود: کلمات قدرت دارند.
با توجه به ارتباط همه چیز، عمل صحبت کردن و کلمات گفته شده، بر محیط اطراف فرد تأثیر می‌گذارد.
این بدان معنی است که آن‌ها نه تنها بر تعاملات فرد تأثیر می‌گذارند، بلکه خود آن‌ها کنش متقابل هستند (یعنی (فعل) کنش.
برای انسان، کلمات روشی برای تعامل با محیط اطراف خود هستند.
بنابراین در وجود هارمونی تأثیر دارند).

واضح است که در مورد زنان فعال سیاسی در هند، توانایی آن‌ها برای خوب زندگی کردن تحت تأثیر استفاده از \textenglish{\textbf{deepfake}} قرار گرفته‌است.
عمل ایجاد \textenglish{\textbf{deepfake}}، و همچنین نحوه‌ی استفاده از آن‌ها، یک عمل دستکاری و اجباری برای وادار کردن زنان به رفتاری است که "در خط" یا برای صاحبان قدرت (مانند مردان و \textenglish{\textbf{BJP}}) مفید باشد.
این کار برای زندگی در هماهنگی با یکدیگر انجام نمی‌شود (این تلاشی است برای تحمیل اراده یک گروه به گروه دیگر و در عین حال ترویج ناهماهنگی).




