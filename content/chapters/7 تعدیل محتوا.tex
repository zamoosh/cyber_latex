%! Author = zamoosh
%! Date = 6/12/23


\chapter{تعدیل محتوا: سخنان نفرت انگیز و نسل کشی در میانمار}
\label{ch:تعدیل محتوا سخنان نفرت انگیز و نسل کشی در میانمار}
\phantomsection

\begin{quote}
    این یک شوخی نژادپرستانه است.
    اینجا مردی است که با یک حیوان مزرعه رابطه جنسی دارد!
    در اینجا یک ویدیوی گرافیکی از قتل ضبط شده توسط یک کارتل مواد‌مخدر است.
    \\\\
    \textbf{کیسی نیوتن، در مورد تعدیل‌محتوا در \textenglish{\textbf{Facebook}}}
    \newline
\end{quote}


{\setstretch{0.5}
\phantomsection
\section*{شرکت \textenglish{\textbf{Facebook}} و پاکسازی قومی در برمه}
\label{sec:شرکت Facebook و پاکسازی قومی در برمه}
\addcontentsline{toc}{section}{شرکت \textenglish{\textbf{Facebook}} و پاکسازی قومی در برمه}{\protect\numberline{}}
در سال 2017، نیروهای نظامی در میانمار سرکوب وحشیانه علیه روهینگیا، یک اقلیت قومی مسلمان ساکن در منطقه غربی این کشور را تشدید کردند. حدود 9000 روهینگیایی توسط نیروهای نظامی به قتل رسیدند و نزدیک به یک میلیون نفر از مرز به بنگلادش گریختند. حدود سه چهارم روهینگیایی‌هایی که در آن زمان در منطقه زندگی می‌کردند شخصاً شاهد یک قتل بودند، یک پنجم شاهد کشتار دسته جمعی بیش از 100 نفر بودند و اکثریت شاهد استفاده نیروهای نظامی از خشونت جنسی علیه زنان روهینگیا به عنوان بخشی از یک کارزار گسترده و سیستماتیک از پاکسازی قومی بودند.
}
نیروهای نظامی که مرتکب این خشونت شدند، دولت منتخب دموکراتیک را در فوریه 2021 سرنگون کردند و به عملیات نظامی خود و همچنین سرکوب مخالفان و آزادی بیان در میانمار ادامه می‌دهند.
ایالات متحده رسما خشونت علیه روهینگیا را نسل‌کشی نامیده‌است و دولت نظامی میانمار از همکاری با تحقیقات دادگاه کیفری بین المللی خودداری کرده است.

از حدود سال 2016، سخنان نفرت‌پراکنی علیه روهینگیاها در \textenglish{\textbf{Facebook}} افزایش یافت که بیشتر آن به حساب‌های کاربری نیروهای نظامی در میانمار مرتبط بود.
بیشتر این سخنرانی شبیه تحریکات خشونت‌آمیز بود که در نسل‌کشی‌های قبلی، از جمله در رواندا در سال 1994 دیده شده‌بود.
هزاران پست وجود‌داشت که به ترویج غیرانسانی کردن مسلمانان روهینگیا و تحریک خشونت علیه آن‌ها، از جمله "شبیه کردن روهینگیاها به حیوانات، و تماس با آن‌ها پرداخته بود." برای کشته شدن روهینگیا، توصیف روهینگیا به عنوان مهاجمان خارجی و به دروغ متهم کردن روهینگیا به جنایات فجیع».
پست‌های دیگر در \textenglish{\textbf{Facebook}} مستقیماً به قتل، تجاوز و آوارگی اجباری روهینگیاها دامن زد.

شواهد واضح بود: حتی «ناتانیل گلیچر، رئیس سیاست امنیت سایبری خود \textenglish{\textbf{Facebook}}»، اعتراف کرد که \textenglish{\textbf{Facebook}} پست‌هایی را تبلیغ می‌کرد که «تلاش‌های واضح و عمدی برای پخش مخفیانه تبلیغاتی بود که مستقیماً به ارتش میانمار مرتبط بود».
ارتش حساب‌های کاربری منتشر کرد و تبلیغاتی را پخش کرد که مخصوصاً برای برانگیختن نفرت طولانی‌مدت قومی علیه روهینگیا طراحی شده بود، از جمله «عکس‌های ساختگی از اجساد که به گفته آن‌ها شواهدی از قتل عام روهینگیا بود».
هدف همه این‌ها توجیه سرکوب نظامی علیه اقلیت قومی مسلمان و بیرون راندن آن‌ها از میانمار بود (که البته با موفقیت بسیار زیادی انجام شد).
\newpage


{\setstretch{0.5}
\phantomsection
\section*{نفرت قدیمی و فناوری جدید}
\label{sec:نفرت قدیمی و فناوری جدید}
\addcontentsline{toc}{section}{نفرت قدیمی و فناوری جدید}{\protect\numberline{}}
نقش \textenglish{\textbf{Facebook}} در نسل کشی میانمار از آن زمان توسط سازمان ملل، تائید شده‌است. این موضوع همچنین موضوع یک شکایت دسته جمعی توسط گروهی از مسلمانان روهینگیا علیه شرکت \textenglish{\textbf{Meta}}، شرکت مادر \textenglish{\textbf{Facebook}}، در کالیفرنیا بوده است. در این شکایت 150 میلیارد دلار به عنوان غرامت و خسارات تنبیهی درخواست شده‌است. این شکایت ادعا می‌کند که \textenglish{\textbf{Facebook}} در حذف سخنان تنفرآمیز که خشونت قومی را تحریک می‌کند، سهل‌انگاری کرده‌است، اما \textenglish{\textbf{Meta}} همچنین ادعا می‌کند که یک ادعای جدید در مورد مسئولیت محصول، به دلیل طراحی معیوب الگوریتم‌های تعدیل‌محتوای \textenglish{\textbf{Facebook}} بوده.
}
تعدیل‌محتوا یکی از سخت‌ترین کارهایی است که شرکت‌های رسانه‌های اجتماعی باید انجام دهند.
\textenglish{\textbf{Facebook}} روزانه میلیاردها پست را تقریباً به هر زبان و فرهنگی در سراسر جهان تعدیل می‌کند.
آن‌ها باید تفاوت‌های ظریف زبانی و فرهنگی را در یک حوزه فرهنگی در حال تغییر در نظر بگیرند (حوزه ای که خود توسط رسانه های اجتماعی و شیوه های تعدیل‌محتوای آن شکل گرفته‌است).
\textenglish{\textbf{Facebook}} سخنان نفرت پراکنی را حذف می کند (حدود 7 میلیون پست در سه‌ماهه سوم سال 2019).
بیش از 80 درصد از سخنان نفرت‌انگیز حذف شده توسط الگوریتم‌های هوش‌مصنوعی (AI) شناسایی شد.
بقیه توسط خود کاربران انجام می‌شوند.
\textenglish{\textbf{Facebook}} بیان می‌کند که تصمیم نهایی برای حذف یک پست به دلیل سخنان مشوق نفرت همیشه توسط یک ناظر انسانی گرفته می‌شود که شناسایی الگوریتم را بررسی می‌کند.

شرکت \textenglish{\textbf{Facebook}}، الگوریتم‌هایی را برای شناسایی سخنان مشوق عداوت و تنفر به بیش از ۴۰ زبان توسعه داده‌است.
در زمان نسل‌کشی روهینگیا، آن‌ها از هوش‌مصنوعی یا ناظر محتوای انسانی برای شناسایی و حذف سخنان نفرت‌انگیز به هیچ یک از زبان‌های رایج در برمه استفاده‌نکردند.
\textenglish{\textbf{Facebook}} همچنین فاقد مدیران محتوای انسانی است که به زبان‌ها و شیوه‌های فرهنگی بسیاری از کشورهای در حال توسعه مسلط هستند (از جمله بسیاری از کشورهای ضعیف که خشونت‌های قومی و سیاسی در آن‌ها شایع است).
این بدان معناست که سخنان نفرت‌انگیز و تحریک به خشونت بیشتر در این کشورها نسبت به کشورهایی مانند اروپا و آمریکای شمالی سریع‌تر گسترش می‌یابد و باعث ایجاد نابرابری‌های سیستماتیک در افرادی می‌شود که در معرض سخنان تنفرآمیز قرار می‌گیرند.
\textenglish{\textbf{Facebook}} از آن زمان برای رفع این مشکل تلاش کرده است: اکنون الگوریتم‌های تعدیل‌محتوا به زبان برمه‌ای را توسعه داده و حدود 100 ناظر محتوای برمه‌زبان (برای کشوری با بیش از 50 میلیون نفر از لحاظ زبانی و قومیتی) استخدام کرده است که به توسعه داده‌های آموزشی بهتر و طبقه بندی گفتارهای نفرت‌انگیز برای \textenglish{\textbf{Facebook}} کمک می‌کنند.

وقتی کسی به تعدیل‌محتوا فکر می‌کند، معمولاً به الگوریتم‌هایی فکر می‌کند که برای بررسی حجم عظیمی از مطالب آنلاین و حذف زیرمجموعه‌ای از محتوای توهین‌آمیز طراحی شده‌اند.
اما این امر از الگوریتم‌های فراگیر که تقریباً همه‌ی مطالب ارسال شده در تمام سایت‌های رسانه‌های اجتماعی را تبلیغ، توصیه و کاهش می‌دهند، در تلاشی بی‌پایان برای جلب مشارکت (و دلارهای تبلیغاتی) نادیده می‌گیرد.
حتی زمانی که مطالب اعتراض آمیز این هدف را ترویج می کند (اما نه زمانی که کاربران را از خود دور می کند، استقبال می‌شود).
یکی از مدیران محتوا برای \textenglish{\textbf{Cognizant}}، یکی از پیمانکاران فرعی \textenglish{\textbf{Facebook}}، به «کیسی نیوتن» گفت که کار مدیران محتوا برای برند \textenglish{\textbf{Facebook}} اساسی است، و اظهار داشت: «اگر ما در آنجا نبودیم و این کار را انجام نمی‌دادیم، \textenglish{\textbf{Facebook}} بسیار زشت بود.
ما داریم می‌بینیم.
ما همه‌ی چیزهایی که از طرف آن‌ها می‌آید را می‌بینیم».

حقیقت این است که همیشه همه‌ی محتوا تعدیل می‌شود.
شکایت علیه \textenglish{\textbf{Meta}} با این ادعا که الگوریتم‌های \textenglish{\textbf{Facebook}} از بازاریابی، روان‌شناسی و علوم اجتماعی برای سوء‌استفاده از آسیب‌پذیری ما در برابر محتوای عاطفی، هیجان‌انگیز، و تفرقه‌انگیز سیاسی استفاده می‌کنند، تلاش می‌کند تا این موضوع را با این ادعا که الگوریتم‌های \textenglish{\textbf{Facebook}} را بیشتر و بیشتر از آن تغذیه می‌کند، جلب کند.
همانطور که «رز استوکول» می‌گوید، «این همان کاری است که رسانه‌های اجتماعی مرتباً با ما انجام می‌دهند: ما را تشویق می‌کند تا درگیری‌ها را مشاهده کنیم و در موضوعاتی که در غیر این صورت نظرات کمی درباره آن‌ها داشتیم، طرف‌هایی را انتخاب کنیم.
در هسته خود، این یک دستگاه خدمات دهی به افکار است.
و در رسانه‌های اجتماعی، همه نظرات به یک اندازه ارائه نمی‌شوند».
نفرت، خشونت (حتی «اخبار جعلی» و انواع اطلاعات نادرست) در مورد آنچه که در رسانه‌های اجتماعی با آن درگیر می‌شویم، مزیت قابل توجهی دارند.
از سوی دیگر، هرچه بیشتر به فیلتر کردن محتوا تشویق می‌کنیم، جریان آزاد گفتار و ایده‌ها را بیشتر زیر پا می‌گذاریم و همان «حباب‌های فیلتر» را که در وهله اول باعث ایجاد مشکل می‌شوند، بیشتر تبلیغ می‌کنیم.

به دلیل تازگی نسبی رسانه‌های اجتماعی در میان مردم، همراه با سانسور شدید میانمار و کمبود منابع اطلاعات، ممکن است نفرت قدیمی و فناوری جدید به شکلی سمی و خطرناک در میانمار با هم برخورد کرده‌باشند.
در سال 2014، کمتر از 1 درصد از مردم به اینترنت دسترسی داشتند و این تعداد تا سال 2018 به حدود 15 میلیون نفر رسید (بیش از یک چهارم جمعیت).
این اتفاق به این دلیل رخ داد که گوشی‌های هوشمند ارزان قیمت با سیم‌کارت‌های 1 دلاری پس از سال 2014 به بازار برمه سرازیر شدند (و تقریباً هر یک از این تلفن‌ها با \textenglish{\textbf{Facebook}} از پیش نصب شده عرضه شدند).
همانطور که ارزیابی حقوق‌بشر از نقش \textenglish{\textbf{Facebook}} در نسل‌کشی روهینگیا بیان کرد، سواد دیجیتال و حاکمیت قانون در برمه بسیار ضعیف بودند.

میانمار درگیر «محدودیت‌های شدید آزادی بیان»، از جمله بازداشت خودسرانه روزنامه‌نگاران، و قوانین سرکوبگر افترا جنایی که برای سرکوب مخالفان طراحی شده‌اند، شناخته شده‌است.
هوگان و سافی گزارش می‌دهند که یک تحلیلگر امنیت سایبری در یانگون اظهار داشت که این منجر به وضعیتی شده‌است که در آن \textenglish{\textbf{Facebook}} مسلماً تنها منبع اطلاعات آنلاین برای اکثریت در میانمار است.
\textenglish{\textbf{Facebook}} می‌دانست که مردم از نظر دیجیتالی ساده لوح هستند، دولت درگیر سرکوب شدید اطلاعات است، و فضای سیاسی مملو از اختلافات قومی است و به شدت مستعد سخنان نفرت‌انگیز و خشونت‌آمیز است.
آن‌ها از این موقعیت برای تقویت تعامل و افزایش درآمدهای تبلیغاتی در برمه استفاده کردند.
همانطور که در شکایت علیه \textenglish{\textbf{Facebook}} آمده است، "\textenglish{\textbf{Facebook}} تصمیم شرکتی گرفت تا به سمت نفرت متمایل شود".

چه شکایت دسته ‌معی علیه \textenglish{\textbf{Meta}} موفقیت‌آمیز باشد یا نه، این بخشی از نقطه‌ی عطف در نحوه نگرش ما به سیستم‌های هوش‌مصنوعی برای تعدیل محتوا است (در نحوه تشخیص و پاسخ به نقص طراحی در قلب این الگوریتم ه)ا.
تعامل، مدل کسب‌وکار رسانه‌های اجتماعی را هدایت می‌کند، زیرا تعامل به معنای لایک، اشتراک‌گذاری و در نتیجه درآمدهای تبلیغاتی است.
پست‌هایی با تعامل بالاتر در فیدهای خبری رسانه‌های اجتماعی بالاتر قرار می‌گیرند.
محتوای نفرت‌انگیز و خشونت‌آمیز توسط تعداد زیادی حساب‌های جعلی تولید و تبلیغ می‌شود، که تعامل بالایی ایجاد می‌کند، و بنابراین الگوریتم‌های \textenglish{\textbf{Facebook}} «آن را در فیدهای خبری کاربران واقعی اولویت‌بندی می‌کنند».
در برمه، این شکایت ادعا می‌کند که الگوریتم‌های فیس‌بوک نه تنها در شناسایی و حذف سخنان نفرت‌انگیز علیه روهینگیا شکست خورده‌اند، بلکه از آن بهره‌برداری کرده و در فیدهای خبری کاربران تبلیغ کرده است.
این تأثیر رادیکالیزه شدن کاربران و «تحمل، حمایت و حتی مشارکت در آزار و اذیت و خشونت قومی» علیه مسلمانان روهینگیا داشت.
\newline
\newline


{\setstretch{0.5}
\phantomsection
\section*{تفسیر}
\label{sec:تفسیر}
\addcontentsline{toc}{section}{تفسیر}{\protect\numberline{}}


\phantomsection
\subsection*{اخلاق بودایی}
\label{subsec:اخلاق بودایی}
\addcontentsline{toc}{subsection}{اخلاق بودایی}{\protect\numberline{}}
\noindent \textbf{نوشته پیتر هرشوک}
\\\\
اتصال دیجیتالی با واسطه محاسباتی، تبدیل تدریجی داده‌های منتقل‌شده با توجه را به جریان‌های درآمد و قدرت برای پیش‌بینی و تولید افکار و رفتار انسانی ممکن می‌سازد. این پتانسیل‌ها از تسریع مصرف مد سریع گرفته تا تقویت اتحادهای سیاسی پوپولیستی، تأثیرگذاری بر رای‌دهندگان نوسان، و دامن زدن به خشونت قومی را شامل می‌شود. گستره این پتانسیل ها از بیهوده تا قاتل به عنوان شاهدی است که اخلاق تعدیل محتوا ساده نیست و نمی‌تواند باشد.
}

به طور مثال، این مطالعه موردی روشن می کند که \textenglish{\textbf{Facebook}} به طور همزمان در میانمار به عنوان یک سرویس خواستگاری غیرسیاسی و درآمدزا برای تولیدکنندگان و مصرف کنندگان، به عنوان بستری برای تحریک خشونت قومی، به عنوان مجرای برای به اشتراک گذاشتن شواهدی از همدستی دولت و ارتش در آن، خشونت، و به عنوان وسیله‌ای برای سازماندهی اعتراض‌ها و مبارزه با سلاح‌سازی احساسات ناامنی شخصی و جمعی، خدمت کرده‌است.

تلاش برای مسئول دانستن \textenglish{\textbf{Facebook}} در قبال خشونتی که روهینگیا متحمل شده است بر اساس منطقی است که به راحتی قابل درک است.
\textenglish{\textbf{Facebook}} در جهت منافع شخصی تجاری خود و با ناآگاهی یا بی توجهی فعال به پتانسیل غم انگیز غفلت از نظارت بر محتوای منتشر شده از طریق پلت فرم خود عمل کرد.
از مسئولیت های اخلاقی خود شانه خالی کرد.

اما آیا شرکت‌ها وظایف اخلاقی دارند؟ اگرچه شرکت‌ها «اشخاص حقوقی» در نظر گرفته می‌شوند، اما کارگزاران اخلاقی سنتی نیستند.
\textenglish{\textbf{Facebook}} شرکتی برای تجاری سازی یک رسانه‌ی دیجیتال یا حوزه ارتباطی است.
ممکن است منطقی باشد که چنین شرکتی مسئولیت فنی جهانی را برای تعدیل محتوا بپذیرد.
اما انتساب مسئولیت‌های اخلاقی محلی موضوع دیگری است زیرا هنجارهای اعتدال مطلوب و مجاز در بین 1.62 میلیارد کاربر روزانه آن بسیار متفاوت است.
می‌توان این بحث را مطرح کرد که همانطور که کشاورزان (و نه مزارع‌شان) هستند که تعیین می‌کنند کدام محصولات را بکارند و بفروشند، این \textenglish{\textbf{Facebook}} نیست که مسئول بذرهای خشونت کاشته‌شده در پلتفرم آن است.
این کسانی هستند که سخنان نفرت‌انگیز را منتشر می‌کنند.

باز هم کشاورزان به تقاضاهای بازار پاسخ می‌دهند، و این قیاس نشان می‌دهد که مسئولیت انتشار سخنان نفرت‌انگیز رفع انسداد در \textenglish{\textbf{Facebook}} را نمی‌توان تنها به کسانی نسبت داد که پست‌های نفرت‌انگیز نوشته‌اند.
بدون اقدامات کسانی که آن‌ها را "لایک" و "به اشتراک گذاشتند"، آن پست ها نمی‌توانست چنین پیامدهای گسترده و خشونت‌آمیزی داشته‌باشد.
آژانس با میانجیگری پلتفرم‌های رسانه‌های اجتماعی، حداقل مسئولیت توزیع شده را پیش‌فرض می‌گیرد.

مسائل زمانی پیچیده‌تر می‌شود که در نظر گرفته شود، در حالی که همه کسانی که به‌عنوان عوامل مستقیم یا غیرمستقیم خشونت و اختلال در زندگی روزمره ناشی از سخنان نفرت‌انگیز منتشر شده در \textenglish{\textbf{Facebook}} درگیر هستند، بیماران آن اقدامات ارتباطی و همه موارد دیگر نیز شده‌اند.
پس از آن‌ها، از جمله تحریم‌های بین المللی، آسیب‌های آبروی و محکومیت اخلاقی، در مجموع، زیرساخت جهانی اتصال دیجیتال نه تنها مرزهای ملی را متخلخل می‌کند، بلکه مرزهای مفهومی را بین عوامل اخلاقی، اعمال و بیماران را نیز محو می‌کند.

هوش‌مصنوعی اغلب به عنوان یک فناوری همه‌منظوره شناخته می‌شود و به طور کلی فرض می‌شود که از نظر اخلاقی، خنثی است.
هر گونه آسیب ناشی از فناوری هوشمند بر عهده‌ی کسانی است که ابزارهای الگوریتمی و یادگیری عمیق را طراحی، استقرار و استفاده می‌کنند.
یعنی این آسیب‌ها به عنوان کارکرد تصادفی طراحی یا استفاده نادرست توسط طراحی در نظر گرفته می‌شوند.
هستی شناسی رابطه‌ای بودایی چیز دیگری را نشان می‌دهد.

برای شروع درک چرایی آن، ابتدا تمایز بین ابزار و فناوری مفید است.
ابزارها مصنوعات قابل بومی‌سازی هستند که ظرفیت‌های ما را برای عمل گسترش می‌دهند یا افزایش می‌دهند و ما به صورت جداگانه آزاد هستیم که از آن‌ها استفاده کنیم یا نه.
فن‌آوری‌ها رسانه‌های رابطه‌ای غیر‌قابل بومی‌سازی هستند که مقاصد و ارزش‌های انسانی را افزایش می‌دهند و به‌طور انتخابی محیط‌هایی را که در آن تصمیم‌گیری می‌کنیم و عمل می‌کنیم تغییر می‌دهند و بر چگونگی و چرایی این کار تأثیر می‌گذارند.
به این معنا که فناوری‌ها از رفتار انسان سرچشمه می‌گیرند و به رفتار انسان‌ها اطلاع می‌دهند/ ساختار می‌دهند، همان‌طور که اکوسیستم‌ها از روابط گونه‌ها بیرون می‌آیند و به صورت بازگشتی اطلاعات/ساختار روابط گونه‌ها را می‌دهند.\ ما «حق خروج» از فناوری را نداریم.


حوادث طراحی و استفاده‌ی نادرست از طریق طراحی، خطرات ابزار هستند.
آسیب‌هایی که توسط الگوریتم‌های تحلیل محتوای محدود یا معیوب \textenglish{\textbf{Facebook}} ممکن شده است، نمونه‌ای از موارد اول است.
استفاده از \textenglish{\textbf{Facebook}} برای ترویج سخنان نفرت‌انگیز و تحریک خشونت‌های قومی نمونه‌هایی از موارد اخیر است.
هر دو اساساً ریسک‌های عامل هستند.
ریسک‌های تکنولوژیکی ساختاری و رابطه‌ای هستند.
برخلاف خطرات ابزار، آن‌ها از شرطی‌سازی پیچیده و بازگشتی روابط انسان-فناوری-جهان سرچشمه می‌گیرند.
خطرات تکنولوژیکی بسیار بیشتر از آسیب‌های محلی است که در مراحل علّی نهایی استفاده از ابزار رخ می‌دهد (نقطه‌ای که در آن عوامل، با یا بدون توجه به بیماران آن اقدام، نیات خود را عملی می‌کنند).
بنابراین، وقتی لابی‌گران حقوق اسلحه استدلال می‌کنند که «اسلحه نمی‌کشد؛ مردم این کار را انجام می‌دهند.» آن‌ها در حال انجام تدبیر مفهومی هستند که به طرز ماهرانه‌ای توجه انتقادی را به سمت ابزارها (تفنگ‌ها) و طراحان و کاربران آن‌ها نادرست هدایت می‌کنند، و از فناوری سلاح دور می‌شوند (یک رسانه‌ی رابطه‌ای که محیط‌های تصمیم‌گیری را به روش‌های مساعد بازسازی می‌کند).
منطقی کردن ایجاد آسیب از راه دور در پاسخ به تهدیدها، توهین‌ها یا تضاد منافع.

اخلاق بودایی، مبتنی بر شناخت منشأ همه چیز، که به یکدیگر وابسته هستند، نشان می‌دهد که پرسیدن اینکه کدام عوامل مسئول نتایج معین هستند، بسیار مهم‌تر از این است که بپرسیم چه ارزش‌ها و نیاتی در شکل‌دادن به پویایی گردش رسانه‌های اجتماعی نقش دارند.
چه الگوهایی از نتایج تجربی و فرصت‌های ارادی توسط رسانه‌های ارتباطی فن‌آوری هوشمند افزایش یافته و به صورت بازگشتی تقویت می‌شوند؟

شرکت \textenglish{\textbf{Facebook}} با انعکاس خاستگاه آمریکایی و لیبرالیسم دره سیلیکون \textenglish{\textbf{(Silicon Valley)}}، آزادی انتخاب و بیان را ارزشمند می‌داند و حکمت اعتدال محتوای حداقلی و صرفاً واکنشی را فرض می‌کند.
پیامدهای استفاده از رسانه‌های اجتماعی در میانمار این موضوع را زیر سؤال می‌برد.
در مقابل، در حالی که چین از تکنیک‌های هوش‌مصنوعی و غربالگری انسانی مشابه \textenglish{\textbf{Facebook}} استفاده می‌کند، سیاست‌ها و شیوه‌های مدیریت محتوای دیجیتال آن بر ارزش‌های ثبات سیاسی و هماهنگی اجتماعی متمرکز است.
علاوه بر این، در حالی که تعدیل محتوای آن هنوز پیشگیری از آسیب را هدف قرار می‌دهد، همچنین هدف آن ایجاد عادات خوب شهروندی از طریق مشوق‌های رفتاری است.
مدیریت محتوا به طور فعال جهت ارتقای رفاه اجتماعی است، همانطور که توسط دولت حزب تعریف شده‌است.

از منظر بودایی، چه رویکرد پیشگیرانه چین به مدیریت محتوا یا رویکرد مینیمالیستی و واکنش‌گرایانه \textenglish{\textbf{Facebook}} برای تعدیل محتوا قابل تحسین یا تأسف باشد، نباید به تأیید یا رد استفاده آن‌ها از ابزارهای یادگیری‌ماشین، صرفه‌جویی در مسئولیت آن‌ها یا حتی آن‌ها بسنده کرد.
تأثیرات کوتاه مدت بر تک تک کاربران رسانه‌های اجتماعی.
این باید به پیامدها و خطرات رابطه میان مدت و بلندمدت آن‌ها بستگی داشته‌باشد.

برخی از راهنمایی‌ها برای این ارزیابی توسط «راه هشت‌گانه بودایی» ارائه می‌شود، که مسیری را به سمت حضور روشنگرانه و روشنگرانه از طریق پرورش دیدگاه‌ها، نیات، گفتار، رفتار، معیشت، تلاش، توجه و تمرکز درست یا اصلاح‌کننده ترسیم می‌کند.
به طور سنتی، گفتار درست، اصلاحی شامل پرهیز از دروغ، غیبت، تهمت، زبان تند، یا توهین آمیز، و همچنین پچ پچ و شایعات بیهوده است.
بسیاری از آنچه توسط رسانه‌های اجتماعی منتشر می‌شود، به وضوح واجد شرایط نیستند.

یک ویژگی مثبت تر از خوب بودن در تمرین بودایی این است که منجر به آغشته کردن تمام موقعیت فرد به ویژگی های رابطه‌ای شفقت، مهربانی، متانت و شادی در خوشبختی دیگران می‌شود.
در حال حاضر، این‌ها توابع هدفی نیستند که سیستم‌های هوش‌مصنوعی در حال حاضر برای بهینه‌سازی هدایت می‌شوند.
اما آیا آن‌ها می‌توانند باشند؟

شرکت \textenglish{\textbf{Facebook}} با موفقیت در دستکاری فیدهای رسانه‌های اجتماعی برای تأثیرگذاری بر احساسات کاربران آزمایش کرده‌است.
خلبانان سیستم اعتبار اجتماعی چین ابزارهای قابل‌قبولی برای تقویت مدنیت عمومی به اثبات رسانده‌اند و محققان ژاپنی سیستم‌های هوش‌مصنوعی را توسعه داده‌اند که به‌طور دقیق احساسات را می‌خواند و انسان‌ها را به روش‌هایی درگیر می‌کند که به عنوان مراقب تجربه می‌شوند.
همه‌ی این‌ها نشان می‌دهد که هیچ مانع فنی برای ایجاد شفقت، مهربانی، متانت، و شادی همدردی به عنوان کارکردهای عینی مدیریت محتوای رسانه‌های اجتماعی مبتنی بر هوش‌مصنوعی وجود ندارد.
اتصال دیجیتال، شاید با طراحی آزاد کننده باشد.
\newpage

{\setstretch{0.5}
\phantomsection
\subsection*{اخلاق فضیلت}
\label{subsec:اخلاق فضیلت}
\addcontentsline{toc}{subsection}{اخلاق فضیلت}{\protect\numberline{}}
\noindent \textbf{نوشته جان هکر رایت}
\\\\
این مورد عواقب واقعاً وحشتناکی را که می‌تواند ناشی از احتمالات جدید برای دستکاری افکار عمومی از طریق رسانه‌های اجتماعی باشد، نشان می‌دهد. نارسایی‌های آشکاری در رهبری شرکت Meta وجود دارد که نوعی بی‌عدالتی را در کسب سود بالاتر از امنیت روهینگیایی که هدف سخنان نفرت‌انگیز قرار گرفته‌اند، به نمایش گذاشت.
}
در این نظر من از این موضع نسبتاً بدبینانه شروع می‌کنم که با وجود تلاش‌های فزاینده از سوی شرکت‌های رسانه‌های اجتماعی، ما به احتمال زیاد شاهد پایان سخنان نفرت‌انگیز و سایر اشکال محتوای دستکاری در پلتفرم‌های رسانه‌های اجتماعی نیستیم.
به هر حال، همانطور که در این فصل اشاره شد، انگیزه‌ی قوی‌ای برای شرکت‌های شبکه‌های اجتماعی برای حفظ و ترویج محتوای تحریک‌آمیز با توجه به اینکه باعث تعامل بیشتر می‌شود، وجود دارد.
اما حتی جدای از آن، این غیرواقعی است که تصور کنیم همه‌ی چنین محتوایی را می‌توان شناسایی و حذف کرد، حتی با بهترین نیت و بودجه‌ی قوی.
با توجه به آن، چه فضیلت‌هایی را می‌توانیم به عنوان کاربران ایجاد کنیم که در برابر چنین دستکاری‌ها محافظت کرده و به طور بالقوه جان انسان‌ها را نجات دهد؟

توجه به این نکته مهم است که حساسیت به چنین دستکاری‌هایی با برخی از ویژگی‌های خوب معامله می‌شود که ما نباید در این فکر کنیم که چگونه خود را در برابر چنین پیامدهای غم انگیزی که در میانمار رخ می‌دهد، قربانی کنیم.
تنها افرادی هستند که نگران شرایط جوامع و ملت خود هستند که می‌توانند بر اساس این نگرانی دستکاری شوند.
دوستی مدنی، که متشکل از احساس مشترک هویت و نگرانی متقابل برای اعضای جامعه است، و میهن پرستی از آنجایی که در کارگزاران با فضیلت دیگر، اعضای یک جامعه را برمی‌انگیزد تا برای یک خیر عمومی اقدام کنند، مسلماً فضیلت هستند.

میهن‌پرستی اغلب با اشتیاق کور به کشور خود همراه است که انگیزه وفاداری بی‌فکر، نگرانی تنگ‌نظرانه‌ی زنجیروارانه و بدرفتاری با بیگانگان است.
اگر میهن‌پرستی به عنوان یک اصطلاح فضیلت به کار می‌رود، این یک اشتیاق نیست، بلکه یک ویژگی مشخص از شخصیت است که عشق ما به کشورمان را مطابق با ویژگی‌های آن تنظیم می‌کند، چیزی مانند غرور مناسب در اعمال خود که با شایستگی‌های واقعی آن‌ها مطابقت دارد.
در کمک به کشورهای همسایه، استقبال از تازه واردان، و مقاومت در برابر عوام فریبی می‌توان غرور میهن‌پرستانه‌ی مناسبی داشت.
میهن‌پرستی همچنین باعث ایجاد احساس شرم در زمانی که کشور فرد ناعادلانه عمل می‌کند.
از این رو، قابل‌قبول است که میهن پرستی را به عنوان یک اصطلاح فضیلت تلقی کنیم، در حالی که اذعان می‌کنیم که این اصطلاح اغلب به این شکل استفاده نمی‌شود.

اما در غیر این صورت می‌توان از ویژگی‌های خوب مانند دوستی مدنی و میهن‌پرستی برای ایجاد انگیزه در اعمال بد از طریق اطلاعات نادرست استفاده کرد.
کسی که عمیقاً نگران جامعه خود است، هنگامی که اطلاعات نادرست در مورد تهدیدی برای جامعه خود دریافت می‌کند، ممکن است در برابر تهدید درک شده ناعادلانه عمل‌کند.
استفاده از نیروی کشنده برای دفاع از جامعه خود در برابر تهدید عموماً از نظر اخلاقی مجاز است.
«توماس آکویناس، فیلسوف و متکلم قرون وسطایی»، که نظریه اخلاقی ارسطو را به تفصیل و نظام مند ساخت، نظریه جنگ عادلانه را توسعه داد که بر اساس آن، تهدید برای جامعه به وضوح شرط کافی برای وارد شدن به درگیری نظامی است (البته نه برای ارتکاب جنایات یک بار علیه دشمن خود، نبرد آغاز شده است).
با این حال آکویناس آنچه را که آشکارا به عنوان پیش‌زمینه در نظریه‌اش فرض می‌شود به صراحت بیان نمی‌کند: قضاوت در مورد تهدید باید از نظر معرفتی صحیح باشد.

در این راستا، فضیلت حکمت عملی بسیار مهم است.
خرد عملی یک فضیلت فکری است که تفکر ما را در مورد آنچه انجام دهیم تنظیم می‌کند.
شخصی با خرد عملی در مورد عمل به خوبی فکر می‌کند.
بخش قابل‌توجهی از استدلال خوب در مورد آنچه که شامل استدلال از مقدمات واقعی است، و دستیابی به مقدمات واقعی، در زندگی عملی آسان تر از زمینه‌های علمی نیست، اما شامل چالش‌های متمایز است.
طبق اخلاق‌فضیلت ارسطویی، یکی از جنبه‌های استدلال از پیش‌فرض‌های واقعی، شامل داشتن فضایل اخلاقی مانند شجاعت است که احساسات ترس و اطمینان ما را تنظیم می‌کند، و اعتدال که اشتهای ما را برای غذا و رابطه جنسی تنظیم می‌کند.

این را در نظر بگیرید: برای کسی که شجاعت ندارد، به نظر می‌رسد یک تهدید بسیار بزرگتر از آن چیزی است که واقعاً هست (ترس او با هدف نامتناسب است).
برای کسی که معتدل است، غذایی که ناسالم است یا متعلق به شخص دیگری است برای خوردن خوب به نظر می‌رسد.
در اینجا ما یک تعهد اساسی ارسطویی را می‌بینیم: در عمل قضاوت‌های ما در مورد جهانی که در آن قرار داریم بر اساس احساسات ما است.
بنابراین، فقدان فضیلت، ادراک ما را تحریف می‌کند و در نتیجه، مقدمات نادرستی را به وجود می‌آورد، به عنوان مثال، «آن مرد، آنجا، بسیار خطرناک است» یا «آن کیک کوچک برای خوردن خوب است».
از آن فرض‌های نادرست، احتمالاً به نتایجی می‌رسیم که منجر به اقدامات بدی می‌شود، مانند «فرار می‌کنم» یا «آن را می‌خورم».
در مورد خشم، کسی که زود عصبانی می‌شود ممکن است به دروغ درباره کسی که شایسته قصاص است قضاوت کند.
البته، استدلال ما همیشه به این صراحت بیان نمی‌شود، و اغلب به روشی سریع و ضمنی، سیستم 1 رخ می‌دهد، اما اگر املا شود، ممکن است چیزی شبیه به بازسازی به‌نظر برسد.
نتیجه این است که در غیاب فضایل اخلاقی، استدلال ما در مورد چگونگی عمل مخدوش خواهد شد.
از این رو، داشتن فضیلت برای پاسخگویی مناسب به موقعیت‌هایی که در آن قرار داریم، از جمله موقعیت‌هایی که از طریق رسانه‌های اجتماعی به ما ارائه می‌شود، مهم است.

تا اینجا، تصویری از استدلال عملی که ارائه کردم تا حد زیادی «ادراکی» است، به این معنا که ما را در حالی نشان می‌دهد که از نقطه نظر خواسته‌هایمان به دنیا نگاه می‌کنیم، و اگر خواسته‌هایمان مرتب باشد، از طریق یک تربیت خوب (ما تمایل داریم که خوب عمل کنیم).
اما داشتن حکمت عملی بیش از داشتن فضایل اخلاقی است.
به گفته ارسطو، افراد دارای خرد عملی «درباره آنچه برای خود و برای انسان خوب است، درک نظری دارند».
این تا حدودی به داشتن دانش‌واقعی اولیه در مورد آنچه در حوزه‌هایی مانند تغذیه خوب است، مربوط می‌شود، اما مهم‌تر از آن، داشتن بینشی در مورد آنچه که از نظر یک زندگی خوب خوب است، است.
به عبارت دیگر، بینش در مورد اینکه چه اعمالی برای یک انسان بهتر است.
ارسطو فکر می کرد که «پریکلس، دولتمرد مشهوری که آتن» را در بخشی از «جنگ پلوپونز» رهبری کرد، چنین مردی است.
کاملاً مشخص نیست که ارسطو چه چیزی را در حمایت از پریکلس می‌ستود، اما در یک سخنرانی معروف در مراسم تشییع جنازه، همانطور که توسیدید گزارش می‌دهد، پریکلس زندگی با مشارکت اجتماعی فعال، دنبال شرافت و به خطر انداختن مرگ به خاطر آزادی را می‌ستاید.
حتی اگر با این تصور از خیر انسانی مخالف باشیم، در برابر برخی دیدگاه‌های کلی در مورد اینکه بهترین نوع فعالیت برای یک انسان چیست، پیشنهادهای خاص مورد ارزیابی قرار‌می‌گیرد و این ادعاهای کلی نیز از جمله مقدمات استدلال ما این است که «برای انسان شایسته است که.
. .» یا «برای انسان بهتر است که.
. .».

یکی دیگر از مؤلفه‌های حکمت عملی، که در این مورد اهمیت ویژه‌ای دارد، یک ظرفیت‌فکری است که ارسطو آن را «درک» می‌نامد، که ظرفیتی است که به وسیله آن موقعیتی را که در آن قرار داریم درک می‌کنیم.
در توضیح این ویژگی، اخلاق‌دان فضیلت ارسطویی «روزالیند هرست هاوس» به موارد زیر اشاره می‌کند:



\begin{quote}
    «موقعیتی» که به انجام کاری نیاز دارد، ممکن است اصلاً با من روبرو نباشد و منتظر باشد تا آن را بخوانم، بلکه چیزی است که باید جزئیات آن را از آنچه دیگران در مورد آن می‌گویند بررسی کنم.
    و تا زمانی که نتوانم قضاوت درستی در مورد گزارش‌های آن‌ها در مورد مسائل مربوطه داشته‌باشم، هر نتیجه عملی‌ای که در مورد اینکه در «این وضعیت» چه کنم، در تاریکی انجام می‌شود.
    \newline
\end{quote}

گزارش هرست هاوس از این جزء از خرد عملی، این را به عنوان یک واقعیت در مورد زندگی انسان تصدیق می کند که ما اغلب در موقعیتی قرار داریم که تصویری از آنچه در جامعه ما اتفاق می‌افتد بر اساس گزارش‌های دیگران از آن بسازیم.
و اغلب دیگران اطلاعات نادرست دارند یا قصد دارند عمداً اطلاعات نادرست را ارائه دهند.
در تشخیص اینکه چه کسی قابل اعتماد است باید درجاتی از زرنگی به دست آوریم.
چنین حساب‌هایی ظاهر یک معضل یا مسیرهای عمل ضروری را در جایی که گزینه‌های بیشتری وجود دارد ایجاد می کند.
ارزیابی حساب‌ها و توانایی رد کامل آن‌ها و انجام تحقیقات بیشتر به تنهایی ظرفیتی حیاتی برای خرد عملی است که توسعه آن به تجربه نیاز دارد.

آنچه در این مورد می‌بینیم، نیاز به بسط «فنی اخلاقی» مفهوم ارسطو از درک است.
بدیهی است که رسانه‌های جمعی و رسانه‌های اجتماعی چالش‌های جدیدی را برای درک مطلب ایجاد می‌کنند که ما را ملزم به توسعه مهارت‌های مناسب برای شناخت تحریف‌ها می‌کند.
ما می‌توانیم سوگیری‌های سیاسی و حس‌گرایی را در رسانه‌های جمعی تشخیص دهیم و تمایلات خود را برای عمل بر اساس آن‌ها قطع کنیم.
به همین ترتیب، می‌توانیم گرایش رسانه‌های اجتماعی به ما را به چیزهایی که قبلاً به آن اعتقاد داریم، ببندیم، دیدگاه‌های متضادی را که ممکن است قضاوت‌های ما را به چالش بکشند، و پناه دادن به حساب‌های ربات جعلی که ما را به نتایجی راهنمایی می‌کنند که در غیر این صورت به آن‌ها نمی‌رسیدیم، برسانند.

در زمینه فناوری‌های نوظهور، مؤلفه حکمت عملی، که ارسطو تنها چند سطر از رساله خود را صرف آن کرده‌است، اهمیت فوق‌العاده‌ای پیدا می‌کند.
درک فنی اخلاقی به عنوان ظرفیتی برای تشخیص اطلاعات قابل‌اعتماد در رسانه های اجتماعی، ظرفیتی است که، به نظر من، حتی در مکان‌هایی که از همان ابتدا رسانه‌های اجتماعی داشته‌اند، به طور گسترده‌ای به نمایش گذاشته نمی‌شود.
توسعه‌ی آن مطمئناً یک فرآیند ناهموار خواهد‌بود، اما به همان اندازه مطمئن است که فضیلت حیاتی که ما در مواجهه با دستکاری گسترده در سیستم عامل‌های رسانه‌های اجتماعی به آن نیاز داریم.
و این چیزی نیست که بتوان آن را در معماری رسانه‌های اجتماعی طراحی کرد.
در هر صورت، اعتماد بیش از حد به الگوریتم‌ها مانع توسعه آن خواهد شد.
پرچم‌هایی که روی داستان‌هایی که مشکوک تلقی می‌شوند قرار می‌گیرند ممکن است ما را به اعتماد نابجا نسبت به داستان‌هایی که چندان پرچم‌دار نیستند، جلب کند.
اگر طراحان پلتفرم‌های رسانه‌های اجتماعی می‌خواهند کمک کنند، شاید بهتر است با تشویق و تأمین مالی تلاش‌های مستقل و تحت رهبری مربیان برای ایجاد مهارت‌های رسانه‌ای تفکر انتقادی که درک فنی اخلاقی را تقویت می‌کنند، انجام‌شود.



{\setstretch{0.5}
\phantomsection
\subsection*{اخلاق بومی}
\label{subsec:اخلاق بومی}
\addcontentsline{toc}{subsection}{اخلاق بومی}{\protect\numberline{}}
\noindent \textbf{توسط جوی میلر و آندریا سالیوان کلارک}
\\\\
ارتباط بین اعتدال محتوا و پاکسازی قومی در میانمار (یعنی برمه) اهمیت اخلاقی این ایده را در فلسفه‌ی بومی نشان می‌دهد که همه‌چیز به هم مرتبط است.
الگوریتم‌ها به خودی خود باعث آسیب نمی‌شوند.
با این حال، نحوه‌ی طراحی و نحوه‌ی استفاده از آن‌ها مطمئناً می‌تواند باعث آسیب شود.
این به دلیل تعداد بی‌شماری از حقایق در مورد نحوه ارتباط و تعامل انسان با محیط اطراف است (به عنوان مثال، ویژگی‌های مختلف روان‌شناختی، بیولوژیکی، اجتماعی، فیزیولوژیکی و غیره انسان و نحوه استفاده از این ویژگی‌ها برای حرکت در محیط اطراف خود).
این بدان معناست که مفاهیم اخلاقی برای نحوه طراحی و استفاده از الگوریتم‌ها وجود دارد.
در حالی که \textenglish{\textbf{Facebook}} خود مستقیماً یا عمداً در قتل و فرار روهینگیایی ها مشارکت نداشته‌است، \textenglish{\textbf{Facebook}} بدون شک مسئولیت این نسل‌کشی را بر عهده دارد.
}

بر اساس درک بومی از اخلاق، مسئولیت \textenglish{\textbf{Facebook}} ناشی از عدم به کارگیری افراد کافی که به زبان برمه صحبت می کنند نیست.
به این ترتیب، استخدام تعداد بیشتری از مدیران محتوای برمه‌زبان این مشکل را برطرف نمی‌کند.
در عوض، استفاده از الگوریتم‌های \textenglish{\textbf{Facebook}} که به‌گونه‌ای طراحی شده‌اند که با بهره‌برداری از ویژگی‌های روان‌شناختی انسان (یعنی بیولوژیکی، اجتماعی و غیره) تعامل را ارتقا دهند، مشکل‌ساز است.
به طور خاص، در مورد عواقب الگوریتم‌هایی که \textenglish{\textbf{Facebook}} استفاده می‌کند، پیش‌بینی یا توجهی وجود ندارد، به این معنی که ارتباط بین این الگوریتم‌ها و تأثیرات آن‌ها (یعنی به هم پیوستگی همه چیز) یا در نظر گرفته نمی‌شود، نادیده گرفته می‌شود یا نادیده گرفته می‌شود.
کنش‌ها به‌عنوان کنش‌های (مقابله) شناخته نمی‌شوند.

حداقل روشن است که توجه کافی به آسیب‌هایی که ممکن است از این نوع الگوریتم‌ها به‌وجود بیاید، صورت نگرفته‌است.
با توجه به افزایش آسیب ناشی از انتشار اطلاعات نادرست، و همچنین ترویج سخنان نفرت‌انگیز و خشونت، واضح است که \textenglish{\textbf{Facebook}} درک کافی از مشتریان خود یا پیامدهای الگوریتم‌های خود نداشته‌است.
از این نظر، \textenglish{\textbf{Facebook}} به برهم‌زدن (یا حداقل برهم زدن بیشتر) تعادل و رفاه در میان مردم و گروه‌های درگیر در نسل‌کشی در میانمار کمک کرد.
به عبارت دیگر، \textenglish{\textbf{Facebook}} به ترویج ناهماهنگی کمک کرد.

با توجه به اینکه، در فلسفه‌ی بومی، (تعامل) کنش‌ها تا حدی درست یا نادرست هستند که هماهنگی را ترویج یا مختل کنند، واضح است که استفاده \textenglish{\textbf{Facebook}} از این الگوریتم‌ها اشتباه است.
با این حال، شیوه‌های «غیرمستقیم» یا «غیر عمدی» \textenglish{\textbf{Facebook}} همچنان به برهم‌زدن هماهنگی کمک می‌کنند.
حتی اگر ناهماهنگی قبلاً در میانمار وجود داشته‌باشد، برهم‌زدن بیشتر هماهنگی همچنان اشتباه است.
این لزوماً استفاده از همه الگوریتم‌ها را به منظور تعدیل محتوا رد نمی‌کند.
این فقط به این معنی است که اگر قرار است از الگوریتم‌ها استفاده‌شود، افرادی که از آن‌ها استفاده می‌کنند باید درک کافی از محدودیت‌ها، دریافت و پیامدهای آن‌ها داشته‌باشند.
بدون چنین درک، تمایل بیشتری برای برهم‌زدن هارمونی وجود دارد.

هیچ قانون قابل اجرا جهانی در مورد چگونگی ارتقای هماهنگی در همه‌ی شرایط وجود ندارد.
در حالی که یک ایده‌ی کلی وجود دارد که هماهنگی باید ترویج شود، چگونگی ارتقای هماهنگی به محیط اطراف فرد (یعنی موقعیت و شرایط آن‌ها) بستگی دارد.
بخشی از انگیزه‌ی پشت این ایده این است که متغیرهای زیادی وجود دارد که در هنگام طراحی نظریه‌های اخلاقی نمی توان آن‌ها را در‌نظر گرفت.
این موضوع در مورد طراحی الگوریتم‌ها نیز صادق است.
الگوریتم‌ها، مانند نظریه‌های اخلاقی، نمی‌توانند برای توضیح همه‌ی موارد احتمالی اشتباه طراحی شوند.
در واقع، همچنین مانند نظریه‌های اخلاقی، می توان از آن‌ها برای ترویج اشتباه استفاده کرد.
این دو ویژگی یا الگوریتم، (1) محدودیت‌های آن‌ها و (2) استفاده از آن‌ها برای ترویج اشتباه، همچنین می‌تواند به تأکید بر اینکه چرا در فلسفه‌ی بومی، هیچ اصل اخلاقی وجود ندارد که در همه موقعیت‌ها یکسان اعمال شود، کمک کند.

شرایط، موقعیت‌ها و زمینه‌ها اهمیت دارد.
بیان فروتنی فکری با آگاهی مناسب از محدودیت‌های یک الگوریتم و اینکه چگونه می‌توان از آن‌ها برای اشتباه استفاده کرد، باید به راهنمایی در مورد نحوه طراحی الگوریتم‌ها یا تصمیم‌گیری از کدام الگوریتم‌ها کمک کند.
در فلسفه‌ی بومی کلمات قدرت دارند.
با تنزل دادن تعدیل (مثلاً ترویج و تنظیم) کلمات به الگوریتم‌ها، نمی‌توان محدودیت‌های این الگوریتم‌ها و/یا قدرت کلمات را تشخیص داد.
به عبارت دیگر، آن‌ها فروتنی نشان نمی‌دهند و این منجر به برهم خوردن هماهنگی یا ترویج ناهماهنگی می‌شود.




