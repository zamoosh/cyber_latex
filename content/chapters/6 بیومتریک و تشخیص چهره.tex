%! Author = zamoosh
%! Date = 6/11/23


\chapter{بیومتریک و تشخیص چهره}
\label{ch:بیومتریک و تشخیص چهره}
\phantomsection


\begin{quote}
    افراد صرفاً با مشارکت در جهان به گونه‌ای که ممکن است چهره خود را برای دیگران آشکار کند یا امکان ثبت تصویر آن‌ها در دوربین را فراهم کند، حق حریم خصوصی خود را نادیده نمی‌گیرند.
    حریم خصوصی برای کرامت، استقلال، رشد شخصی و مشارکت آزاد و آزاد افراد در زندگی دموکراتیک حیاتی است.
    هنگامی که نظارت افزایش می‌یابد، افراد می‌توانند از اعمال این حقوق و آزادی‌ها منصرف شوند.
    \\\\
    \textbf{دانیل ترین \textenglish{\textbf{(Daniel Therrien)}}، کمیسر حریم خصوصی کانادا}
    \newline
\end{quote}


{\setstretch{0.5}
\phantomsection
\section*{شرکت \textenglish{\textbf{Clearview AI}}}
\label{sec:شرکت Clearview AI}
\addcontentsline{toc}{section}{شرکت \textenglish{\textbf{Clearview AI}}}{\protect\numberline{}}
استثمار جنسی آنلاین از کودکان (چیزی مانند پورنوگرافی برای کودکان) یک مشکل جدی و رو به رشد در سراسر جهان است. بین سال‌های 2014 تا 2019، گزارش‌ها به پلیس سواره سلطنتی کانادا \textenglish{\textbf{(RCMP)}} از عکس‌ها یا ویدیوهایی که آزار جنسی کودکان را به تصویر می‌کشند، حدود \textenglish{\textbf{1106\%}}  افزایش یافته است. در سال 2019، \textenglish{\textbf{RCMP}} بیش از 102967 گزارش از سوء‌استفاده جنسی آنلاین از کودکان دریافت‌کرد. با توجه به مقیاس مشکل، مکان یابی، شناسایی و حذف مطالب ممکن است دشوار باشد.
}

این می‌تواند به ویژه برای قربانیان سخت باشد.
مطالب مستهجن که سوء‌استفاده از آن‌ها را به تصویر می‌کشد می‌تواند سال‌ها در اینترنت باقی‌بماند.
در فصل 5 ما بحث کردیم که چگونه فیلم‌های مستهجن جعلی برای روزنامه نگاران زن در هند آسیب زا بوده است و اینکه ارسال چنین مطالبی به عنوان ابزاری برای خشونت سیاسی علیه زنان در آن کشور استفاده شده‌است.
برای قربانیان سوء‌استفاده جنسی از کودکان، ویدیوهایی که آن‌ها را در شرایط تحقیرآمیز و توهین‌آمیز جنسی نشان می‌دهد واقعی است و به نظر نمی‌رسد که هرگز ازبین نرود.

چندین سازمان غیرانتفاعی شروع به استفاده از فناوری‌های یادگیری‌ماشین برای مشکل شناسایی و حذف مطالبی که آزار جنسی کودکان را به تصویر می‌کشد، کرده‌اند.
در ایالات متحده، سازمان غیرانتفاعی \textenglish{\textbf{Thorn}} ابزاری به نام \textenglish{\textbf{Spotlight}} ایجاد کرده است که از فناوری تشخیص چهره \textenglish{\textbf{(FRT)}} برای شناسایی قربانیان سوء‌استفاده جنسی و قاچاق کودکان استفاده می‌کند.
افسران مجری قانون می‌توانند عکس یک کودک گم شده یا مورد استثمار را آپلود کنند و سپس به جستجوی ویدیوهایی بپردازند که کودک را به تصویر می‌کشد، یا تبلیغات آنلاینی را که به کودک پیشنهاد رابطه جنسی می‌دهد.
در کانادا، \textenglish{\textbf{Project Arachnid}} از هوش‌مصنوعی \textenglish{\textbf{(AI)}} برای جستجوی تصاویر سوءاستفاده استفاده جنسی از کودکان در اینترنت عادی و حتی دارک‌وب \textenglish{\textbf{(dark web)}} استفاده می‌کند و سپس درخواست‌های حذف را برای حذف مطالب صادر می‌کند.
آن‌ها حدود 6 میلیون عکس و ویدیو را از وب حذف کرده‌اند و هر روز تعداد بیشتری پست می‌شود.

در اکتبر سال 2019، مرکز ملی جرایم بهره‌کشی از کودکان \textenglish{\textbf{(NCECC)}} (بخشی از \textenglish{\textbf{RCMP}}) شروع به استفاده از فناوری تشخیص‌چهره برای شناسایی کودکان قربانی استثمار جنسی آنلاین کرد.
آن‌ها دو مجوز از یک شرکت آمریکایی به نام \textenglish{\textbf{Clearview AI}} خریداری کردند که به آن‌ها امکان دسترسی به الگوریتم‌های تشخیص چهره \textenglish{\textbf{Clearview}} و دیتابیس‌های عظیم عکس‌ها را می‌داد.
آن‌ها همچنین از چندین حساب آزمایشی رایگان استفاده کردند که توسط \textenglish{\textbf{Clearview}} به سازمان‌های مجری قانون ارائه شده‌بود.
\textenglish{\textbf{RCMP}} از این فناوری در مقر ملی و همچنین در بریتیش کلمبیا، آلبرتا، مانیتوبا و نیوبرانزویک استفاده کرد.

مرکز \textenglish{\textbf{NCECC}} بیان می‌کند که از فناوری تشخیص چهره \textenglish{\textbf{Clearview}} در 15 مورد استفاده کرده و 2 کودک را نجات داده‌است.
علاوه بر این، \textenglish{\textbf{Clearview}} حدود 14 بار برای شناسایی عاملی که از اجرای قانون فرار می‌کرد استفاده شد.
\textenglish{\textbf{RCMP}} بیان می‌کند که در غیر این صورت از این فناوری به صورت آزمایشی استفاده می‌کند تا ببیند فناوری تشخیص‌چهره چه کاربردی می‌تواند در پیشبرد تحقیقات جنایی به طور کلی داشته‌باشد.
کمیسیونر حریم خصوصی بیان می‌کند که \textenglish{\textbf{RCMP}} هدف اکثر صدها جستجوی انجام شده را فاش نکرده است.
پلیس تورنتو همچنین از \textenglish{\textbf{Clearview}} در چندین تحقیق در همان زمان استفاده کرد.
مانند \textenglish{\textbf{RCMP}}، استفاده مجاز نبود یا مشمول هیچ گونه کنترل‌داخلی نبود.
در پایان، در 84 تحقیق بین اکتبر 2019 و فوریه 2020، که بیشتر آن‌ها مربوط به قتل و جنایات جنسی بود، استفاده شد:آن‌ها 12 قربانی (10 نفر ازآن‌ها کودک) و همچنین 2 شاهد و 4 مظنون را شناسایی کردند.
هیئت خدمات پلیس تورنتو بیان می‌کند که برنامه‌ای برای استفاده مجدد از \textenglish{\textbf{Clearview}} ندارد و اخیراً سیاستی را برای استفاده از فناوری تشخیص چهره وضع کرده‌است.
مانند سرویس پلیس تورنتو، \textenglish{\textbf{RCMP}} در ابتدا استفاده از \textenglish{\textbf{Clearview}} را برای کمیسر حریم خصوصی کانادا رد کرد.
سپس دفتر کمیسر حریم خصوصی تحقیقاتی را در مورد اینکه آیا استفاده از \textenglish{\textbf{Clearview}} قوانین حریم خصوصی کانادا را نقض می‌کند، آغاز کرد.
چندین نگرانی توسط کمیسر حریم خصوصی مطرح شد، از جمله این واقعیت که فناوری تشخیص چهره ممکن است علیه فعالان و معترضان به کار گرفته‌شود، اینکه این فناوری پتانسیل "فناوری نظارتی بسیار تهاجمی" را دارد، و اینکه استفاده از آن ممکن است سایر انسان‌های اساسی را نقض کند.
حقوق، از جمله با ترویج تبعیض نژادی در سیستم عدالت کیفری.
\newpage


{\setstretch{0.5}
\phantomsection
\section*{بیومتریک و فناوری تشخیص چهره}
\label{sec:بیومتریک و فناوری تشخیص چهره}
\addcontentsline{toc}{section}{بیومتریک و فناوری تشخیص چهره}{\protect\numberline{}}
یکی از انواع فناوری بیومتریک است (که امروزه بسیاری از آن‌ها با یادگیری ماشین تسهیل شده‌اند). شناسایی‌های بیومتریک از سایر اشکال شناسه، مانند رمزهای عبور، کارت‌ها و سایر نشانه‌ها، ایمن‌تر هستند همه‌ی آن‌ها ممکن است گم شوند، دزدیده شوند، جعل شوند یا در معرض حملات آزمایشی و خطا قرار گیرند (که خود توسط سیستم‌های هوش مصنوعی که توسط بازیگران بد به کار می‌روند آسان‌تر می‌شوند). یک بیومتریک قابل اعتماد هم بسیار فردی خواهد بود (باید به طور دقیق بین یک فرد و فرد دیگر تمایز قائل شود) و هم در طول زمان پایدار است، به طوری که یک فرد بتواند در صورت نیاز به استفاده از هویت خود تکیه کند. دو مورد از قابل اعتمادترین و منحصربه‌فردترین بیومتریک‌ها، «اثر‌انگشت» و «اسکن عنبیه چشم» هستند و احتمالاً به همین دلیل برای برنامه \textenglish{\textbf{Aadhaar}} هند انتخاب شده‌اند. با این حال، قابل اعتماد بودن به معنای بی خطا بودن نیست. اسکن عنبیه برای افراد مسن و افراد مبتلا به آب مروارید دشوار است. اثر انگشت برای حدود 1 تا 3 درصد از جمعیت یک شکل غیرقابل اعتماد از شناسه است و همانطور که بسیاری از هندی‌ها به ضرر خود کشف کرده‌اند، باید با تغییر اثر انگشت ما در طول عمرمان به‌روزرسانی شوند، در حالی که برای دیگران ممکن است به طور کامل تغییر یابند یا از بین‌بروند. در عین حال، چیزی که بیومتریک را به یکی از مطلوب‌ترین و مطمئن‌ترین شکل‌های شناسایی تبدیل می‌کند، این واقعیت است که آن‌ها بخشی جدایی‌ناپذیر از شخصیت ما به‌عنوان افراد منحصربه‌فرد هستند؛ دقیقاً همان چیزی است که اطلاعات آن‌ها را بسیار حساس می‌کند.
}

فناوری تشخیص‌چهره در مقایسه با اسکن اثر انگشت و عنبیه، روشی کمتر قابل اعتماد برای شناسایی است.
فناوری‌های تشخیص‌چهره به طور مستقیم چهره ما را اندازه گیری یا تجزیه و تحلیل نمی‌کنند.
آن‌ها "هیچ تصوری از یک شخص خاص ندارند" و "برای شناسایی افراد خاص ساخته نشده‌اند." در عوض، آن‌ها فواصل مشخصی را بین اجزای صورت اندازه می‌گیرند: فاصله بین چشم‌های ما، عرض بینی، عمق حدقه‌های چشم، طول خط فک و غیره.
این اندازه‌گیری‌ها یک مقدار برداری تولید می‌کنند که به عنوان یک پروکسی (نماینده) برای یک فرد معین عمل می‌کند.
سپس باید بین مقادیر برداری تصاویر مختلف مقایسه‌شود.
مقایسه را می‌توان برای اهداف ثبت‌نام انجام داد، به عنوان مثال، هنگام گرفتن عکس برای ثبت‌نام در یک برنامه تشخیص چهره که به فرد امکان دسترسی به یک مکان امن، یک دستگاه الکترونیکی یا دریافت خدماتی مانند یک داروی تجویزی را می‌دهد.
مقایسه همچنین می‌تواند بر اساس «یک به یک» باشد، مانند زمانی که یک فرد از تصویری از چهره‌ی خود استفاده می‌کند تا بعداً به برنامه دسترسی پیدا کند.
جستجوها همچنین می‌توانند «یک به چند» باشند که شامل مقایسه عکس فرد با پایگاه‌داده عکس‌های مشابه است.
سپس مقدار برداری ایجاد شده از تصویر فرد با تصویر موجود در پایگاه داده مقایسه می‌شود تا احتمال اینکه تصاویر مربوط به همان شخص باشد، مشخص شود.
شناسایی موقت زمانی انجام می‌شود که احتمال از مقدار آستانه معینی فراتر رود.
این توسط یک اپراتور انسانی که تصاویر را مقایسه می‌کند بررسی می‌شود، اما این می‌تواند نوعی سوگیری تایید را در فرآیند شناسایی معرفی کند.
اپراتور انسانی به احتمال زیاد یکی از نامزدها را انتخاب می‌کند و کاندیدایی را که بالاترین رتبه را دارد انتخاب می‌کند زیرا معتقد است الگوریتم بسیار مؤثر است.
استفاده پلیس از تصاویر برای جستجوی مظنونان، و استفاده \textenglish{\textbf{RCMP}} از \textenglish{\textbf{Clearview AI}} برای جستجوی قربانیان جرایم جنسی آنلاین شامل جستجوهای یک به چند از این نوع است.
\newline
\newline


{\setstretch{0.5}
\phantomsection
\section*{تشخیص چهره و حفظ حریم خصوصی}
\label{sec:تشخیص چهره و حفظ حریم خصوصی}
\addcontentsline{toc}{section}{تشخیص چهره و حفظ حریم خصوصی}{\protect\numberline{}}
کمیسیونر حریم خصوصی دریافت که \textenglish{\textbf{Clearview AI}} با جمع‌آوری اطلاعات خصوصی کانادایی‌ها بدون رضایتآن‌ها با جمع‌آوری یک پایگاه داده تصویری از حدود 10 میلیارد عکس برای مقایسه، قوانین حریم خصوصی کانادا را نقض کرده است.
\textenglish{\textbf{Clearview}} پاسخ داد که این عکس‌ها که از اینترنت و سایت‌های رسانه‌های اجتماعی حذف شده‌اند، هنگام انتشار در دسترس عموم قرار گرفته‌اند.
\textenglish{\textbf{RCMP}}، به نوبه خود، بر اظهارات \textenglish{\textbf{Clearview}} مبنی بر اینکه عکس‌ها "عمومی" شده اند، تکیه کرد.
کمیسیونر حریم خصوصی دریافت که \textenglish{\textbf{RCMP}} باید رضایت افراد را برای استفاده از تصاویر آن‌ها در تحقیقات خود جلب می‌کرد.
}

در پایان، \textenglish{\textbf{RCMP}} موافقت کرد که سیاست‌های خود را تغییر‌دهد و از توصیه‌های کمیسر حریم خصوصی تبعیت کند.
در آن زمان، نرم‌افزار در ونکوور، ادمونتون، کلگری و اتاوا آزمایش شده‌بود.
همچنین در فرانسه، ایالات متحده، استرالیا، و بریتانیا مورد استفاده قرار گرفته‌است، جایی که موضوع بحث و جدل‌های فراوان و چندین اقدام قانونی در مورد پتانسیل آن برای نظارت جمعی و جستجوها و توقیف‌های غیرمنطقی بوده است.
این واقعیت که داده‌های مورد استفاده برای آموزش و اجرای الگوریتم بیومتریک توسط کاربران عمومی شده است، به دانشمندان داده این حق را نمی‌دهد که فرض کنند صاحبان آن داده‌ها با استفاده از آن موافقت کرده‌اند.
\newline
\newline




{\setstretch{0.5}
\phantomsection
\section*{تفسیر}
\label{sec:تفسیر}
\addcontentsline{toc}{section}{تفسیر}{\protect\numberline{}}


\phantomsection
\subsection*{اخلاق بومی}
\label{subsec:اخلاق بومی}
\addcontentsline{toc}{subsection}{اخلاق بومی}{\protect\numberline{}}
\noindent \textbf{توسط جوی میلر و آندریا سالیوان کلارک}
\\\\
بیومتریک و فناوری تشخیص‌چهره این پتانسیل را دارند که ابزاری برای سرکوب شوند. این امر به ویژه زمانی اتفاق می‌افتد که هیچ نظارت یا سیاستی در مورد استفاده از آن‌ها وجود نداشته‌باشد. بدون چنین راهنمایی، این فناوری ممکن است به طور خودسرانه علیه جوامع به حاشیه رانده شده، به ویژه جوامعی که وضعیت موجود را به چالش می‌کشند، استفاده‌شود. در غیاب نظارت خارجی و ایجاد سیاست‌های مربوط به استفاده از بیومتریک، نگرانی اصلی افراد و جوامع بومی، امکان هدف قرار گرفتن برای نظارت است. این یک نگرانی مشروع است (این اولین بار نیست که از واکنش‌های نظامی علیه مردم بومی استفاده می‌شود). تاریخ‌های اخیر ایالات متحده و کانادا مملو از نمونه‌هایی از پاسخ‌های نظامی شده به اعتراضات بومی است، مانند بحران اوکا، سنگ ایستاده، و محاصره‌ی \textenglish{\textbf{«Wet’suwet’en»}}. استعمار را می‌توان به عنوان جنگ توصیف کرد و روش‌هایی که دولت‌های فدرال برای حفظ وضعیت موجود به کار می‌برند، نشان‌دهنده این موضوع است. با این حال، نگرانی در مورد نظارت و از دست دادن حریم خصوصی فرد در مطالعه موردی، چارچوب غربی این موضوع است.
}

برای درک اخلاقیات استفاده از بیومتریک و فناوری تشخیص‌چهره به گونه‌ای که با جهان‌بینی بومی سازگار باشد، مستلزم چارچوب بندی مجدد بحث بر حسب روابطی است که به جای بحث حقوق فردی درگیر است.
به عنوان مثال، استفاده از بیومتریک و فناوری تشخیص‌چهره بر روابط بین مردم بومی و دولت فدرال تأثیر می‌گذارد.
وقتی دولت‌ها بر حقیقت و آشتی تأکید می‌کنند در حالی که آژانس‌های آن‌ها از فناوری برای نظارت بر افراد و جوامع بومی استفاده می‌کنند، بر روابط پرتنش تاریخی تأثیر منفی می‌گذارد.
زمانی که اقدامات و سخنان هر یک از طرفین سازگار نباشد، هر دو طرف در روابط متضرر می‌شوند: کار برای استعمارزدایی برای مردم بومی سخت‌تر می‌شود و دولت‌ها (مانند ایالات متحده و کانادا) نه تنها چهره‌ی خود را در صحنه جهانی از دست می‌دهند.
و برای مردم بومی، اما آن‌ها همچنین نمی‌توانند از رویکردهای متنوع حل مسئله که ممکن است یک جهان‌بینی بومی ارائه دهد، بهره‌مند شوند.

بیومتریک و فناوری تشخیص‌چهره، از دیدگاه بومی، به خودی خود غیراخلاقی نیستند.
در عوض، نحوه استفاده از فناوری (به عنوان مثال، آیا استفاده از آن به تعادل و هماهنگی کمک می‌کند؟ یا استفاده از آن باعث ارتقای روابط خوب می‌شود؟) نحوه ارتباط فرد با آن را تعیین می‌کند.
فقدان نظارت و سیاست‌های مربوط به نحوه استفاده از فناوری، امکان استفاده خودسرانه علیه جوامع حاشیه‌نشین را فراهم می‌کند.
نه تنها باید یک سیاست استفاده ایجاد شود، بلکه باید با مشورت مردم و جوامع بومی انجام شود.
از دیدگاه غربی، \textenglish{\textbf{Clearview AI}} یک ابزار است و کاربرد آن به استفاده از آژانس‌های پلیس دولتی محدود می‌شود.
مشورت با جوامع بومی ممکن است کاربردهای دیگری نیز داشته‌باشد، مانند بازگرداندن تعادل با رسیدگی به موارد متعدد زنان و دختران بومی گم‌شده و کشته‌شده \textenglish{\textbf{(MMIWG)}}. کار در همکاری با مردم بومی به رابطه اعتماد کمک می‌کند و در عین حال حاکمیت و خودمختاری ملت‌های بومی را حفظ می‌کند.

موضوع دیگری که نیاز به بررسی دارد این است که برخی از اقدامات بیومتریک، مانند اثر انگشت و فناوری تشخیص‌چهره، ممکن است باعث ایجاد اعتماد کاذب در شناسایی افرادی شود که گمان می‌رود مرتکب جرم شده‌اند.
اعتماد بیش از حد با توجه به استفاده از این فناوری نشان‌دهنده عدم تواضع (یک ارزش بومی) است.
داشتن چنین اعتمادی بر مردم بومی تأثیر منفی خواهد گذاشت، زیرا آن‌ها در حال حاضر بیش از حد در میزان زندانی شدن در ایالات متحده و کانادا حضور دارند.
جستجوهایی که بر مقایسه یک عکس با عکس‌های موجود در پایگاه‌داده تکیه می‌کنند، عینیت فردی را که مقایسه می‌کند، زیر سؤال می‌برد.
افرادی که دارای ویژگی‌های فنوتیپی مرتبط با گروه‌های نژادی هستند در برابر تعصبات و تعصبات ضمنی کسانی که تصمیم می‌گیرند آسیب پذیرتر می‌شوند.
فروتنی ممکن است برای کاهش مسئله اعتماد به نفس بیش از حد مورد استفاده قرار گیرد.
\newline
\newline



{\setstretch{0.5}
\phantomsection
\subsection*{بشردوستی و قوانین درگیری مسلحانه}
\label{subsec:بشردوستی و قوانین درگیری مسلحانه}
\addcontentsline{toc}{subsection}{بشردوستی و قوانین درگیری مسلحانه}{\protect\numberline{}}
\noindent \textbf{نوشته تریسی داودزول}
\\\\
علیرغم چالش‌های قانونی، هوش مصنوعی \textenglish{\textbf{Clearview}} به طور فعال به دنبال اولین قراردادهای بزرگ دولت ایالات متحده است (به ویژه با سازمان‌های مجری قانون فدرال مانند \textenglish{\textbf{FBI}}، اداره مهاجرت و گمرک، و خدمات ماهی و حیات وحش). آن‌ها همچنین در حال تحقیق در مورد استفاده از تشخیص‌چهره و واقعیت افزوده برای ایمن‌سازی پست‌های بازرسی پایگاه نیروی هوایی هستند.
}

علیرغم آزمایش نشده‌بودن در درگیری‌های مسلحانه، هوش‌مصنوعی \textenglish{\textbf{Clearview}} به طور رسمی فناوری خود را برای استفاده در زمان جنگ در 10 مارس 2022 در اوکراین عرضه کرد.
وزارت دفاع اوکراین استفاده از هوش‌مصنوعی \textenglish{\textbf{Clearview}} را ظاهراً برای شناسایی کشته‌شدگان، مبارزه با اطلاعات نادرست و «دامپزشکی افراد مورد‌علاقه در پست‌های بازرسی» آغاز کرد.

علیرغم آزمایش نشده بودن در درگیری‌های مسلحانه، هوش‌مصنوعی \textenglish{\textbf{Clearview}} به طور رسمی فناوری خود را برای استفاده در زمان جنگ در 10 مارس 2022 در اوکراین عرضه کرد.
وزارت دفاع اوکراین استفاده از هوش‌مصنوعی \textenglish{\textbf{Clearview}} را ظاهراً برای شناسایی کشته‌شدگان، مبارزه با اطلاعات نادرست و «دامپزشکی افراد مورد علاقه در پست‌های بازرسی» آغاز کرد.
با توجه به مستعد بودن سیستم‌های تشخیص‌چهره به خطا و سوگیری، استفاده از آن علیه غیرنظامیان به‌ویژه نگران‌کننده است، زیرا مثبت کاذب (منظور از مثبت کاذب، این است که یک فرایند یا موضوع که کاملا صحیی و بی‌آزار است، به خاطر شباهت ظاهری به موارد غلط و نادرست، به عنوان مورد مشکوک شناسایی بشود) ممکن است منجر به بازداشت‌های نادرست یا حتی قتل در نقض قوانین بین‌المللی جنگ شود.
اگر این سیستم غیرنظامیان را در ایست‌های بازرسی، یا در داخل و اطراف سایت‌های نبرد شناسایی کند، ممکن است منجر به ارتکاب جنایات جنگی شود.

تمایز غیرنظامیان از جنگجویان در زمان جنگ (از جمله غیرنظامیانی که ممکن است به عنوان مبارزان مقاومت، جاسوسان، شورشیان یا سایر نیروهای چریکی عمل کنند) و اینکه چگونه باید با این موضوع در قوانین بین المللی جنگ برخورد شود، مشکلی دیرینه است.
وضعیت فعلی قانون این است که نیروهای نظامی تنها تا زمانی می‌توانند چنین غیرنظامیانی را هدف قرار‌دهند که آن‌ها به طور فعال تهدید مسلحانه باشند.
هر گونه فعالیت دیگری باید با یک محاکمه عادلانه بر اساس قوانین داخلی برای افرادی که حق ندارند به عنوان اسیران جنگی رفتار شوند، رسیدگی‌شود.
در 150 سال گذشته، کشورها از اسناد بین‌المللی بشردوستانه مانند کنوانسیون‌های لاهه و ژنو و پروتکل‌های الحاقی آن‌ها عقب‌نشینی کرده‌اند تا به آن‌ها آزادی عمل بیشتری برای شناسایی و کشتن غیرنظامیانی بدهند که فکر می‌کنند ممکن است تهدیدی برای تلاش‌های جنگی آن‌ها باشد.

استفاده از فناوری تشخیص‌چهره در یک منطقه‌ی جنگی قطعاً این پتانسیل را دارد که این مشکل طولانی مدت را تشدید کند و جنایات جنگی را تسهیل کند.
یک غیرنظامی که به یک ایست بازرسی نزدیک می‌شود، یا به دنبال کمک‌های بشردوستانه است، ممکن است به دلیل تطابق مثبت کاذب از پایگاه‌داده تشخیص‌چهره، مورد هدف نیروهای مسلح قرار گیرد.
هنگامی که این فناوری به ارتش اوکراین ارائه شد، هیچ تلاشی برای اطمینان از عدم استفاده از آن برای ارتکاب جنایات جنگی از این نوع انجام نشد.
در واقع، خود \textenglish{\textbf{Clearview AI}} خاطرنشان کرد که "هدف دقیقی که وزارت دفاع اوکراین از این فناوری برای آن استفاده می‌کند نامشخص است".
مدیر عامل \textenglish{\textbf{Clearview}}، \textenglish{\textbf{Hoan Ton-That}}، اظهار داشت که او هرگز نمی‌خواهد ببیند که این فناوری در تضاد با کنوانسیون ژنو است و هرگز نباید از آن به عنوان تنها منبع شناسایی استفاده شود، اما او هیچ قاعده یا حفاظتی را وضع نکرده است، که از این اتفاق جلوگیری کند.

قوانین بین‌المللی جنگ به طور کلی «اکتساب یا اتخاذ» روش‌ها و ابزارهای جنگی جدید را ممنوع می‌کند، مگر اینکه دولت بتواند ابتدا تعیین کند که «به‌کارگیری آن در برخی شرایط یا در همه‌ی شرایط، توسط این پروتکل یا هر قاعده‌ی دیگری منع می‌شود یا خیر".
استفاده از فناوری تشخیص‌چهره در درگیری مسلحانه بین‌المللی جدید است و با توجه به خطرات بسیار بالای استفاده از فناوری‌های تشخیص‌چهره برای شناسایی افراد ناشناس در زمان واقعی، همراه با پتانسیل اثبات شده‌ی آن‌ها برای خطاها و سوگیری جمعیتی، ما استدلال می‌کنیم که به طور کلی باید به عنوان یک ابزار ممنوعه جنگ در نظر گرفته‌شود.
ما توصیه می‌کنیم که قوانین بین‌المللی به گونه‌ای تفسیر شود که شناسه‌های بیومتریک مانند فن‌آوری‌های تشخیص‌چهره را فقط می‌توان در درگیری‌های مسلحانه بین‌المللی برای اهداف بشردوستانه، مانند شناسایی متوفیان و افراد آواره و پیوستن مجدد آن‌ها به خانواده‌هایشان، به کار برد.



